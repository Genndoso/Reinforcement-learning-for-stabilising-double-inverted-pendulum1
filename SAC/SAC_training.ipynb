{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a4c65dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from SAC import Actor, Critic, ReplayBuffer, Value\n",
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Максимилиан\\\\Desktop\\\\Skoltech\\\\Reinforcement learning\\\\Final project')\n",
    "from Dynamics import get_next_state, state_to_coords, get_energy,normalize_angle\n",
    "from Environment import DoublePendulumEnv, ObservationSpaceCartPole\n",
    "from config_SAC import config\n",
    "from Agent import Agent\n",
    "from utils import plot_learning_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cec4a9d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5bc2dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0.0001,np.pi/2,np.pi/2,\n",
    "                0.0001,0.0001,0])\n",
    "state = state0\n",
    "max_initial_angle = 0\n",
    "dt = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c92e0f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Максимилиан\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float64\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = DoublePendulumEnv(init_state=state, dt=config['dt'], max_initial_angle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09912b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join('SAC', config['env_name'])\n",
    "if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e85d72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "937ce97f",
   "metadata": {},
   "outputs": [],
   "source": [
    "buffer = ReplayBuffer(config['memory_size'], env.observation_space.shape, env.action_space.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c0685c6",
   "metadata": {},
   "outputs": [],
   "source": [
    " agent = Agent(gamma= config['gamma'], alpha=config['alpha'], beta=config['beta'], state_dims=env.observation_space.shape,\n",
    "                  action_dims=env.action_space.shape, max_action=env.action_space.high[0],\n",
    "                  fc1_dim=config['fc1_dim'], fc2_dim=config['fc2_dim'], memory_size=config['memory_size'],\n",
    "                  batch_size=config['batch_size'], tau=config['tau'], update_period=config['update_period'],\n",
    "                  reward_scale=config['reward_scale'], warmup=config['warmup'], reparam_noise_lim=config['reparam_noise_lim'],\n",
    "                  name='SAC_'+config['env_name'], ckpt_dir=dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "70c77c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Максимилиан\\Desktop\\Skoltech\\Reinforcement learning\\Final project\\SAC\\Agent.py:72: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_new.cpp:204.)\n",
      "  state = T.tensor([state], dtype=T.float).to(self.actor.device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Game:      0 | Score:  111573.21 | Best score:       -inf | Avg score  111573.21 | Learning inter:             31 |\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "... saving checkpoint ...\n",
      "| Game:      1 | Score:   82906.85 | Best score:  111573.21 | Avg score   97240.03 | Learning inter:             59 |\n",
      "| Game:      2 | Score:   92602.28 | Best score:  111573.21 | Avg score   95694.11 | Learning inter:             87 |\n",
      "| Game:      3 | Score:   87698.92 | Best score:  111573.21 | Avg score   93695.32 | Learning inter:            115 |\n",
      "| Game:      4 | Score:   86596.23 | Best score:  111573.21 | Avg score   92275.50 | Learning inter:            143 |\n",
      "| Game:      5 | Score:   88993.25 | Best score:  111573.21 | Avg score   91728.46 | Learning inter:            171 |\n",
      "| Game:      6 | Score:   95298.82 | Best score:  111573.21 | Avg score   92238.51 | Learning inter:            200 |\n",
      "| Game:      7 | Score:   95559.53 | Best score:  111573.21 | Avg score   92653.64 | Learning inter:            229 |\n",
      "| Game:      8 | Score:   81287.57 | Best score:  111573.21 | Avg score   91390.74 | Learning inter:            257 |\n",
      "| Game:      9 | Score:   89722.15 | Best score:  111573.21 | Avg score   91223.88 | Learning inter:            285 |\n",
      "| Game:     10 | Score:   88707.14 | Best score:  111573.21 | Avg score   90995.09 | Learning inter:            313 |\n",
      "| Game:     11 | Score:   86917.08 | Best score:  111573.21 | Avg score   90655.25 | Learning inter:            341 |\n",
      "| Game:     12 | Score:   92445.05 | Best score:  111573.21 | Avg score   90792.93 | Learning inter:            370 |\n",
      "| Game:     13 | Score:   83381.58 | Best score:  111573.21 | Avg score   90263.55 | Learning inter:            398 |\n",
      "| Game:     14 | Score:   95868.22 | Best score:  111573.21 | Avg score   90637.19 | Learning inter:            427 |\n",
      "| Game:     15 | Score:   84168.89 | Best score:  111573.21 | Avg score   90232.92 | Learning inter:            455 |\n",
      "| Game:     16 | Score:   92795.68 | Best score:  111573.21 | Avg score   90383.67 | Learning inter:            484 |\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], grad_fn=<AddmmBackward0>), scale: tensor([[nan]], grad_fn=<ClampBackward1>)) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], grad_fn=<AddmmBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     13\u001b[0m     action \u001b[38;5;241m=\u001b[39m agent\u001b[38;5;241m.\u001b[39mchoose_action(observation, deterministic\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, reparameterize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 15\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreparameterize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m observation_, reward, done, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     17\u001b[0m score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n",
      "File \u001b[1;32m~\\Desktop\\Skoltech\\Reinforcement learning\\Final project\\SAC\\Agent.py:80\u001b[0m, in \u001b[0;36mAgent.choose_action\u001b[1;34m(self, state, deterministic, reparameterize)\u001b[0m\n\u001b[0;32m     77\u001b[0m     actions \u001b[38;5;241m=\u001b[39m mu\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;66;03m# stochastic action is sampled from the normal distribution\u001b[39;00m\n\u001b[1;32m---> 80\u001b[0m     actions, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreparameterize\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m actions\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32m~\\Desktop\\Skoltech\\Reinforcement learning\\Final project\\SAC\\SAC.py:124\u001b[0m, in \u001b[0;36mActor.sample_normal\u001b[1;34m(self, state, reparameterize)\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample_normal\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, reparameterize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    122\u001b[0m     mu, sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mforward(state)\n\u001b[1;32m--> 124\u001b[0m     probs \u001b[38;5;241m=\u001b[39m \u001b[43mNormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reparameterize:\n\u001b[0;32m    126\u001b[0m         actions \u001b[38;5;241m=\u001b[39m probs\u001b[38;5;241m.\u001b[39mrsample()\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\distributions\\normal.py:54\u001b[0m, in \u001b[0;36mNormal.__init__\u001b[1;34m(self, loc, scale, validate_args)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     batch_shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloc\u001b[38;5;241m.\u001b[39msize()\n\u001b[1;32m---> 54\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mNormal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidate_args\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\distributions\\distribution.py:55\u001b[0m, in \u001b[0;36mDistribution.__init__\u001b[1;34m(self, batch_shape, event_shape, validate_args)\u001b[0m\n\u001b[0;32m     53\u001b[0m         valid \u001b[38;5;241m=\u001b[39m constraint\u001b[38;5;241m.\u001b[39mcheck(value)\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m valid\u001b[38;5;241m.\u001b[39mall():\n\u001b[1;32m---> 55\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     56\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected parameter \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     57\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(value)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(value\u001b[38;5;241m.\u001b[39mshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     58\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mof distribution \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     59\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto satisfy the constraint \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mrepr\u001b[39m(constraint)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     60\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbut found invalid values:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mvalue\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28msuper\u001b[39m(Distribution, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n",
      "\u001b[1;31mValueError\u001b[0m: Expected parameter loc (Tensor of shape (1, 1)) of distribution Normal(loc: tensor([[nan]], grad_fn=<AddmmBackward0>), scale: tensor([[nan]], grad_fn=<ClampBackward1>)) to satisfy the constraint Real(), but found invalid values:\ntensor([[nan]], grad_fn=<AddmmBackward0>)"
     ]
    }
   ],
   "source": [
    "scores, avg_scores = [], []\n",
    "best_score = -np.inf\n",
    "\n",
    "for game in range(config['n_games']):\n",
    "        observation, done = env.reset()\n",
    "\n",
    "        score = 0\n",
    "        counter = 0\n",
    "        \n",
    "        for i in range(0,301):\n",
    "            \n",
    "            if config['play']:\n",
    "                action = agent.choose_action(observation, deterministic=True, reparameterize=False)\n",
    "            else:\n",
    "                action = agent.choose_action(observation, deterministic=False, reparameterize=False)\n",
    "            observation_, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "            agent.store_transition(observation, action, reward, observation_, done)\n",
    "            if not config['play']:\n",
    "                agent.learn()\n",
    "            observation = observation_\n",
    "            counter += 1\n",
    "            if done:\n",
    "                break\n",
    "        scores.append(score)\n",
    "        avg_score = np.mean(scores[-100:])\n",
    "        avg_scores.append(avg_score)\n",
    "\n",
    "        print(f'| Game: {game:6.0f} | Score: {score:10.2f} | Best score: {best_score:10.2f} | '\n",
    "              f'Avg score {avg_score:10.2f} | Learning inter: {agent.learn_iter:14.0f} |')\n",
    "\n",
    "        if avg_score > best_score:\n",
    "            best_score = avg_score\n",
    "            if not config['play']:\n",
    "                agent.save_model()\n",
    "env.close()\n",
    "\n",
    "if not config['play']:\n",
    "    plot_learning_curve(scores, agent.full_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b93e9458",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710b6a35",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
