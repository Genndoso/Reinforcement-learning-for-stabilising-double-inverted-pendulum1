{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab825263",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Максимилиан\\\\Desktop\\\\Skoltech\\\\Reinforcement learning\\\\Final project')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d530e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casadi import *\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "from Dynamics2 import get_next_state, state_to_coords, get_energy, normalize_angle\n",
    "from Environment import DoublePendulumEnv #, normalize_angle\n",
    "from PPO.Proximal_Policy_Optimization import PPO, unscaled_action\n",
    "from PPO.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221d43d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dynamics2 import m_real, l_real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9d877f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "from matplotlib import animation\n",
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ead7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvPendulumEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, init_state, dt=0.02, max_initial_angle = 3 * 2 * np.pi / 360):\n",
    "        self.action_space = 1\n",
    "        self.observation_space = 6\n",
    "        self.state = init_state\n",
    "        self.init_state = init_state\n",
    "        self.dt = dt\n",
    "        print('Environment initialized')\n",
    "        self.init_coords = state_to_coords(init_state)\n",
    "        self.max_initial_angle = max_initial_angle\n",
    "    def _take_action(self, action):\n",
    "        self.state = get_next_state(self.state, action, self.dt)\n",
    "\n",
    "    def _reward_function(self, done):\n",
    "        \"\"\"\n",
    "\n",
    "        # Reward system 1\n",
    "        Check whether 1 and 2 cart pole are in angle range between 80 and 100 degrees\n",
    "        agent will agent a reward in range [0, 1]\n",
    "\n",
    "        else:\n",
    "            If angle of pole 1 and 2  are greater than 10 degrees, therefore, it makes sense\n",
    "            to terminate the environment and reset/restart.\n",
    "            agent will get a  reward = -1\n",
    "\n",
    "        # Reward system 2\n",
    "        If cart is in given range of x = [-5, 5] then agent will get a reward 0.5 every steps.\n",
    "        Otherwise, it penalies the system heavily of a penalty = -50 and system is done here.\n",
    "\n",
    "        # Reward system 3\n",
    "        # this is unused. This an analog to reward system 1 but for coordinates\n",
    "        If cart pole is not in the same line with cart then it will give additional penalty\n",
    "\n",
    "\n",
    "        # Reward system 4\n",
    "        Velocity penalty (halves the reward if spinning too fast)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        reward = 0\n",
    "        # degree reward\n",
    "        normalized_angle_1 = np.degrees((state[1]))\n",
    "       # normalized_angle_2 = np.degrees((state[2]))\n",
    "\n",
    "        if normalized_angle_1 > 80 and normalized_angle_1 < 10:\n",
    "            reward = 1 - (90 - normalized_angle_1) * 0.01\n",
    "            reward *= 3\n",
    "            \n",
    "       \n",
    "        else: \n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "#        # another degree reward system\n",
    "#         cost = (normalize_angle(state[1]) - np.pi/2) #+ \\\n",
    "#                   #     2(normalize_angle(state[2]) - np.pi/2)\n",
    "\n",
    "#         reward += cost\n",
    "#         if normalized_angle_1 > 85 and normalized_angle_1 < 95:\n",
    "#             reward += 1 - (90 - normalized_angle_1) * 0.01\n",
    "\n",
    "        # another degree_reward system\n",
    "\n",
    "        #         deg_reward = ((np.sin(state[1]))*10 + (np.sin(state[2]))*10)/2\n",
    "        #         #if np.sin(state[1]\n",
    "        #         reward += deg_reward\n",
    "        #         print(state[1])\n",
    "\n",
    "        # distance penalty\n",
    "        if state[0] < 2 and state[0] > -2:\n",
    "            pass\n",
    "        else:\n",
    "            reward -= -50\n",
    "            done = True\n",
    "\n",
    "       # distance2 rew\n",
    "#         state_coords = state_to_coords(state)\n",
    "#                # dist_pen = (state_coords[0][1] - state_coords[0][0])**2 +  (state_coords[0][2] - state_coords[0][0])**2\n",
    "#         dist_rew =  -( state_coords[1][1] - self.init_coords[1][1]) -  ( state_coords[1][2] - self.init_coords[1][2])*2\n",
    "#         reward -= dist_rew\n",
    "        \n",
    "\n",
    "#        # velocity penalty\n",
    "#         vel_pen = ((1 + np.exp(-0.5 * state[-3:] ** 2)) / 2).sum()*10\n",
    "#         reward -= vel_pen\n",
    "        \n",
    "#         print(-vel_pen)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        observation -  [x,phi,theta,dx,dphi,dtheta]\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -2 m                  2 m\n",
    "        1       Pole1 Angle               -pi                     +pi\n",
    "        2       Pole2 Angle               -pi                     +pi\n",
    "        3       Cart Velocity             -Inf                    Inf\n",
    "        4       Pole1 Angular Velocity    -Inf                    Inf\n",
    "        5       Pole1 Angular Velocity    -Inf                    Inf\n",
    "\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        info = {}\n",
    "        self._take_action(action)\n",
    "\n",
    "       \n",
    "        reward, done = self._reward_function(done)\n",
    "        return np.array(self.state), reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Compute the render frames as specified by render_mode attribute during initialization of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        ani = animation.FuncAnimation(fig, animate, frames=300,\n",
    "                                      interval=20, blit=True, init_func=init)\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to an initial state and returns the initial observation.\n",
    "        \"\"\"\n",
    "        self.rew_sum = 0\n",
    "        self.state = self.init_state\n",
    "        d = np.random.uniform(-self.max_initial_angle, self.max_initial_angle)\n",
    "        self.state[1] = np.pi/2 + np.random.uniform(-self.max_initial_angle, self.max_initial_angle)\n",
    "        self.state[2] = 0\n",
    "\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1785b48",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state\n",
    "#max_initial_angle = 3 * 2 * np.pi / 360\n",
    "max_initial_angle = 0\n",
    "dt = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a8a2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = InvPendulumEnv(init_state = state, dt = 0.02, max_initial_angle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1cd08997",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"============================================================================================\")\n",
    "    # max. timestep per episode. For DoubleCartPoleEnv, time constraint is 200 timesteps. After that environment is \n",
    "    # reset.\n",
    "    directory_plots = \"1_pole_PPO_plots\"\n",
    "    if not os.path.exists(directory_plots):\n",
    "          os.makedirs(directory_plots)\n",
    "            \n",
    "    writer = SummaryWriter(log_dir = directory_plots )\n",
    "    max_ep_len = 300\n",
    "    # The training phase will sample and update for 1 million timestep.\n",
    "    max_training_steps = int(1e6)\n",
    "\n",
    "    # In order, to check ongoing progress, average reward is printed at every 10_000 timesteps.\n",
    "    print_freq = 10_000\n",
    "    \n",
    "    # Saving model parameters at every 1_00_000 timesteps.\n",
    "    save_model_freq = int(1e5)\n",
    "\n",
    "    action_std = 0.4                                    # Initial standard deviation.\n",
    "    action_std_decay_rate = 0.1                       # Decay rate of standard deviation.\n",
    "    min_action_std = 0.1                                # Threshold standard deviation.\n",
    "    action_std_decay_freq = int(2e5)                    # Decay the standard deviation every 2_00_000 timesteps\n",
    "\n",
    "    update_timestep = 2000                              # set old_policy parameters to new_policy parameters.\n",
    "    K_epochs = 100                                      # Number of epochs before updating old policy parameters.\n",
    "    eps_clip = 0.2                                      # clip range for surrogate loss function.\n",
    "    gamma = 0.99                                        # Discount factor.\n",
    "\n",
    "    lr_actor = 3e-4                                   # Learning rate for optimizer of actor network.\n",
    "    lr_critic = 0.001                                  # Learning rate for optimizer of critic network.\n",
    "    env_name = 'One_Pole_InvPendulum'\n",
    "    print(\"Training Environment:\" + env_name)\n",
    "    env = InvPendulumEnv(init_state = state, dt = 0.02, max_initial_angle = 0)\n",
    "\n",
    "    observation_shape = 6  # Observation shape\n",
    "    action_shape = 1          # Action shape\n",
    "\n",
    "    # Creating a directory to store the model parameters during and after training.\n",
    "    directory = \"1_pole_PPO3_Trained\"\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/'\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "    \n",
    "    checkpoint_path = directory + \"PPO3_{}.pth\".format(env_name)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"max training timesteps : \", max_training_steps)\n",
    "    print(\"max timesteps per episode : \", max_ep_len)\n",
    "    print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "    print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"state space dimension : \", observation_shape)\n",
    "    print(\"action space dimension : \", action_shape)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "    print(\"PPO K epochs : \", K_epochs)\n",
    "    print(\"PPO epsilon clip : \", eps_clip)\n",
    "    print(\"discount factor (gamma) : \", gamma)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"optimizer learning rate actor : \", lr_actor)\n",
    "    print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)\n",
    "\n",
    "    print(\"Starting the Training\")\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # To keep track of the progress\n",
    "    print_running_reward = 0    \n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    counter = 0\n",
    "\n",
    "    plot_episode = []\n",
    "    plot_reward = []\n",
    "\n",
    "    while time_step <= max_training_steps:\n",
    "        obs = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action = agent.select_action(obs)                           # Get action under old_policy given state.\n",
    "            action = unscaled_action(action, action_low = -100, action_high = 100)# Unscale the action.\n",
    "            obs, reward, done, _ = env.step(action)                     # Apply the action to environment.\n",
    "            \n",
    "\n",
    "            # Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)                             \n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                # Perform updates using sampled data.\n",
    "                agent.update()\n",
    "\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                # Decay standard deviation by 0.1.\n",
    "                agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward during 10_000 timesteps\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "\n",
    "                # Save the model parameters for test phase and tracking performance.\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Saving model at : \" + checkpoint_path)\n",
    "                agent.save(checkpoint_path)\n",
    "                print(\"Model saved\")\n",
    "                counter += 1\n",
    "                inter_checkpoint = directory + \"PPO_{}_{}00K.pth\".format(env_name, counter)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")               \n",
    "                print(\"Model parameters to check for intermediate performance saving:.\")\n",
    "                print(\"saving model at : \" + inter_checkpoint)\n",
    "                agent.save(inter_checkpoint)\n",
    "                if counter == 10:\n",
    "                    print(f\"Intermediate model saved for {counter}M\")   \n",
    "                else:\n",
    "                    print(f\"Intermediate model saved for {counter}00K\")                \n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        \n",
    "        # plot reward per 10 episode.\n",
    "        if i_episode % 10 == 0:\n",
    "            writer.add_scalar(\"Episode reward\", current_ep_reward,i_episode)\n",
    "            plot_episode.append(i_episode)\n",
    "            plot_reward.append(current_ep_reward)\n",
    "            \n",
    "\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Training Finished\")\n",
    "\n",
    "    \n",
    "    return plot_episode, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0855f97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Training Environment:One_Pole_InvPendulum\n",
      "Environment initialized\n",
      "save checkpoint path : 1_pole_PPO3_Trained/PPO3_One_Pole_InvPendulum.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  1000000\n",
      "max timesteps per episode :  300\n",
      "model saving frequency : 100000 timesteps\n",
      "printing average reward over episodes in last : 10000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  6\n",
      "action space dimension :  1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.2\n",
      "decay rate of std of action distribution :  0.1\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 200000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 2000 timesteps\n",
      "PPO K epochs :  100\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Starting the Training\n",
      "============================================================================================\n",
      "Episode : 115 \t\t Timestep : 10000 \t\t Average Reward : 30.02\n",
      "Episode : 236 \t\t Timestep : 20000 \t\t Average Reward : 33.38\n",
      "Episode : 353 \t\t Timestep : 30000 \t\t Average Reward : 34.91\n",
      "Episode : 467 \t\t Timestep : 40000 \t\t Average Reward : 34.77\n",
      "Episode : 584 \t\t Timestep : 50000 \t\t Average Reward : 31.58\n",
      "Episode : 694 \t\t Timestep : 60000 \t\t Average Reward : 25.2\n",
      "Episode : 809 \t\t Timestep : 70000 \t\t Average Reward : 31.38\n",
      "Episode : 926 \t\t Timestep : 80000 \t\t Average Reward : 27.15\n",
      "Episode : 1045 \t\t Timestep : 90000 \t\t Average Reward : 29.21\n",
      "Episode : 1157 \t\t Timestep : 100000 \t\t Average Reward : 17.95\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : 1_pole_PPO3_Trained/PPO3_One_Pole_InvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : 1_pole_PPO3_Trained/PPO_One_Pole_InvPendulum_100K.pth\n",
      "Intermediate model saved for 100K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 1275 \t\t Timestep : 110000 \t\t Average Reward : 26.77\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_episode, reward_episode = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa34d3c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b393ddb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Plot: Reward per every 10 episodes.\n",
    "fig, ax = plt.subplots(figsize=(20, 10))\n",
    "ax.plot(n_episode, reward_episode)\n",
    "ax.set_title('Total number of episodes vs rewards per episode', fontsize=20)\n",
    "ax.set_xlabel('Episode', fontsize=20)\n",
    "ax.set_ylabel('Reward', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7a020fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#checkpoint_path = 'PPO_DoubleInvPendulum_0_0.pth'\n",
    "observation_shape = 6\n",
    "action_shape = 1\n",
    "lr_actor = 3e-4  # Learning rate for optimizer of actor network.\n",
    "lr_critic = 0.001  # Learning rate for optimizer of critic network.\n",
    "gamma = 0.99\n",
    "action_std = 0.6  # Initial standard deviation.\n",
    "action_std_decay_rate = 0.1  # Decay rate of standard deviation.\n",
    "min_action_std = 0.1  # Threshold standard deviation.\n",
    "action_std_decay_freq = int(2e5)  # Decay the standard deviation every 2_00_000 timesteps\n",
    "\n",
    "update_timestep = 5000  # set old_policy parameters to new_policy parameters.\n",
    "K_epochs = 50  # Number of epochs before updating old policy parameters.\n",
    "eps_clip = 0.2  # clip range for surrogate loss function.\n",
    "gamma = gamma  # Discount factor.\n",
    "\n",
    "agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7803b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = 'One_Pole_InvPendulum'\n",
    "directory = \"1_pole_PPO3_Trained\" + '/' \n",
    "checkpoint_path = directory + \"PPO_{}_100K.pth\".format(env_name)\n",
    "print(\"loading network from : \" + checkpoint_path)\n",
    "agent.load(checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9488dfd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "state = state0\n",
    "dt = 0.02\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                     xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "energy_text = ax.text(0.02, 0.90, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    #time_text.set_text('')\n",
    "    energy_text.set_text('')\n",
    "    return line\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"perform animation step\"\"\"\n",
    "    global state, dt\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    u = agent.select_action(state_t)\n",
    "    action = unscaled_action(u, action_low = -10, action_high = 10)\n",
    "    state = get_next_state(state,u,dt)\n",
    "    XY = state_to_coords(state)\n",
    "    en = get_energy(state)\n",
    "    \n",
    "    line.set_data(XY[0],XY[1])\n",
    "    energy_text.set_text(f'energy = {en}')\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=300,\n",
    "                             interval=20, blit=True, init_func=init)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a28bfae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
