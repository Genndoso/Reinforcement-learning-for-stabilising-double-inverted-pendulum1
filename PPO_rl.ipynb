{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4aaacb5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dynamics import get_next_state, state_to_coords, get_energy\n",
    "from Environment import DoublePendulumEnv, normalize_angle\n",
    "from PPO.Proximal_Policy_Optimization import PPO, unscaled_action\n",
    "from PPO.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc1084ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fce57c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3193da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi/2,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state\n",
    "max_initial_angle = 3 * 2 * np.pi / 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cedb7b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[1] = np.pi/2 + np.random.uniform(-max_initial_angle, max_initial_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa2de66c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.02395701300198"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.degrees(state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddac38f5",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20232529",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"============================================================================================\")\n",
    "    # max. timestep per episode. For DoubleCartPoleEnv, time constraint is 200 timesteps. After that environment is \n",
    "    # reset.\n",
    "    max_ep_len = 300\n",
    "    # The training phase will sample and update for 1 million timestep.\n",
    "    max_training_steps = int(1e6)\n",
    "\n",
    "    # In order, to check ongoing progress, average reward is printed at every 10_000 timesteps.\n",
    "    print_freq = 10_000\n",
    "    \n",
    "    # Saving model parameters at every 1_00_000 timesteps.\n",
    "    save_model_freq = int(1e5)\n",
    "\n",
    "    action_std = 0.6                                    # Initial standard deviation.\n",
    "    action_std_decay_rate = 0.1                       # Decay rate of standard deviation.\n",
    "    min_action_std = 0.1                                # Threshold standard deviation.\n",
    "    action_std_decay_freq = int(2e5)                    # Decay the standard deviation every 2_00_000 timesteps\n",
    "\n",
    "    update_timestep = 4000                              # set old_policy parameters to new_policy parameters.\n",
    "    K_epochs = 100                                      # Number of epochs before updating old policy parameters.\n",
    "    eps_clip = 0.2                                      # clip range for surrogate loss function.\n",
    "    gamma = 0.99                                        # Discount factor.\n",
    "\n",
    "    lr_actor = 3e-3                                     # Learning rate for optimizer of actor network.\n",
    "    lr_critic = 0.001                                   # Learning rate for optimizer of critic network.\n",
    "    env_name = 'DoubleInvPendulum'\n",
    "    print(\"Training Environment:\" + env_name)\n",
    "    env = DoublePendulumEnv(init_state = state, dt = 0.02, max_initial_angle = max_initial_angle)\n",
    "\n",
    "    observation_shape = 6  # Observation shape\n",
    "    action_shape = 1          # Action shape\n",
    "\n",
    "    # Creating a directory to store the model parameters during and after training.\n",
    "    directory = \"PPO2_Trained\"\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/'\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "    \n",
    "    checkpoint_path = directory + \"PPO2_{}.pth\".format(env_name)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"max training timesteps : \", max_training_steps)\n",
    "    print(\"max timesteps per episode : \", max_ep_len)\n",
    "    print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "    print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"state space dimension : \", observation_shape)\n",
    "    print(\"action space dimension : \", action_shape)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "    print(\"PPO K epochs : \", K_epochs)\n",
    "    print(\"PPO epsilon clip : \", eps_clip)\n",
    "    print(\"discount factor (gamma) : \", gamma)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"optimizer learning rate actor : \", lr_actor)\n",
    "    print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)\n",
    "\n",
    "    print(\"Starting the Training\")\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # To keep track of the progress\n",
    "    print_running_reward = 0    \n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    counter = 0\n",
    "\n",
    "    plot_episode = []\n",
    "    plot_reward = []\n",
    "\n",
    "    while time_step <= max_training_steps:\n",
    "        obs = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action = agent.select_action(obs)                           # Get action under old_policy given state.\n",
    "            action = unscaled_action(action)                            # Unscale the action.\n",
    "            obs, reward, done, _ = env.step(action)                     # Apply the action to environment.\n",
    "         \n",
    "\n",
    "            # Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)                             \n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                # Perform updates using sampled data.\n",
    "                agent.update()\n",
    "\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                # Decay standard deviation by 0.1.\n",
    "                agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward during 10_000 timesteps\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "\n",
    "                # Save the model parameters for test phase and tracking performance.\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Saving model at : \" + checkpoint_path)\n",
    "                agent.save(checkpoint_path)\n",
    "                print(\"Model saved\")\n",
    "                counter += 1\n",
    "                inter_checkpoint = directory + \"PPO_{}_{}00K.pth\".format(env_name, counter)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")               \n",
    "                print(\"Model parameters to check for intermediate performance saving:.\")\n",
    "                print(\"saving model at : \" + inter_checkpoint)\n",
    "                agent.save(inter_checkpoint)\n",
    "                if counter == 10:\n",
    "                    print(f\"Intermediate model saved for {counter}M\")   \n",
    "                else:\n",
    "                    print(f\"Intermediate model saved for {counter}00K\")                \n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        \n",
    "        # plot reward per 10 episode.\n",
    "        if i_episode % 10 == 0:\n",
    "            plot_episode.append(i_episode)\n",
    "            plot_reward.append(current_ep_reward)\n",
    "\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Training Finished\")\n",
    "\n",
    "    \n",
    "    return plot_episode, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be3f9936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Training Environment:DoubleInvPendulum\n",
      "Environment initialized\n",
      "save checkpoint path : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  1000000\n",
      "max timesteps per episode :  300\n",
      "model saving frequency : 100000 timesteps\n",
      "printing average reward over episodes in last : 10000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  6\n",
      "action space dimension :  1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.6\n",
      "decay rate of std of action distribution :  0.1\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 200000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 4000 timesteps\n",
      "PPO K epochs :  100\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Starting the Training\n",
      "============================================================================================\n",
      "Episode : 36 \t\t Timestep : 10000 \t\t Average Reward : -390.87\n",
      "Episode : 74 \t\t Timestep : 20000 \t\t Average Reward : -353.06\n",
      "Episode : 117 \t\t Timestep : 30000 \t\t Average Reward : -297.42\n",
      "Episode : 163 \t\t Timestep : 40000 \t\t Average Reward : -263.32\n",
      "Episode : 204 \t\t Timestep : 50000 \t\t Average Reward : -299.97\n",
      "Episode : 242 \t\t Timestep : 60000 \t\t Average Reward : -302.29\n",
      "Episode : 282 \t\t Timestep : 70000 \t\t Average Reward : -304.22\n",
      "Episode : 323 \t\t Timestep : 80000 \t\t Average Reward : -289.83\n",
      "Episode : 363 \t\t Timestep : 90000 \t\t Average Reward : -288.15\n",
      "Episode : 405 \t\t Timestep : 100000 \t\t Average Reward : -262.09\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_100K.pth\n",
      "Intermediate model saved for 100K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 448 \t\t Timestep : 110000 \t\t Average Reward : -262.21\n",
      "Episode : 490 \t\t Timestep : 120000 \t\t Average Reward : -265.85\n",
      "Episode : 529 \t\t Timestep : 130000 \t\t Average Reward : -277.25\n",
      "Episode : 568 \t\t Timestep : 140000 \t\t Average Reward : -313.68\n",
      "Episode : 612 \t\t Timestep : 150000 \t\t Average Reward : -238.04\n",
      "Episode : 658 \t\t Timestep : 160000 \t\t Average Reward : -208.99\n",
      "Episode : 705 \t\t Timestep : 170000 \t\t Average Reward : -208.5\n",
      "Episode : 754 \t\t Timestep : 180000 \t\t Average Reward : -183.61\n",
      "Episode : 800 \t\t Timestep : 190000 \t\t Average Reward : -223.74\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.5\n",
      "Episode : 844 \t\t Timestep : 200000 \t\t Average Reward : -219.75\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_200K.pth\n",
      "Intermediate model saved for 200K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 891 \t\t Timestep : 210000 \t\t Average Reward : -202.41\n",
      "Episode : 943 \t\t Timestep : 220000 \t\t Average Reward : -193.1\n",
      "Episode : 994 \t\t Timestep : 230000 \t\t Average Reward : -192.2\n",
      "Episode : 1050 \t\t Timestep : 240000 \t\t Average Reward : -144.13\n",
      "Episode : 1104 \t\t Timestep : 250000 \t\t Average Reward : -172.92\n",
      "Episode : 1159 \t\t Timestep : 260000 \t\t Average Reward : -155.98\n",
      "Episode : 1207 \t\t Timestep : 270000 \t\t Average Reward : -202.88\n",
      "Episode : 1258 \t\t Timestep : 280000 \t\t Average Reward : -167.87\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_episode, reward_episode = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ccba51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Reward per every 10 episodes.\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(n_episode, reward_episode)\n",
    "ax.set_title('Total number of episodes vs rewards per episode', fontsize=20)\n",
    "ax.set_xlabel('Episode', fontsize=20)\n",
    "ax.set_ylabel('Reward', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27602b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "state = state0\n",
    "\n",
    "dt = 0.1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                     xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "energy_text = ax.text(0.02, 0.90, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    #time_text.set_text('')\n",
    "    energy_text.set_text('')\n",
    "    return line\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"perform animation step\"\"\"\n",
    "    global state, dt\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    u = agent.select_action(state_t)\n",
    "    action\n",
    "    state = get_next_state(state,u[0].detach().numpy(),dt)\n",
    "    XY = state_to_coords(state)\n",
    "    en = get_energy(state)\n",
    "    \n",
    "    line.set_data(XY[0],XY[1])\n",
    "    energy_text.set_text(f'energy = {en}')\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=100,\n",
    "                             interval=10, blit=True, init_func=init)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e28369",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
