{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6757250f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dynamics import get_next_state, state_to_coords, get_energy\n",
    "from Environment import DoublePendulumEnv, normalize_angle\n",
    "from PPO.Proximal_Policy_Optimization import PPO, unscaled_action\n",
    "from PPO.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3008303",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2db0ae0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fd1d0955",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi/2,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state\n",
    "max_initial_angle = 3 * 2 * np.pi / 360"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a57f6c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "state[1] = np.pi/2 + np.random.uniform(-max_initial_angle, max_initial_angle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "30435f36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "91.70596646873625"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.degrees(state[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78033bbd",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7020d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"============================================================================================\")\n",
    "    # max. timestep per episode. For DoubleCartPoleEnv, time constraint is 200 timesteps. After that environment is \n",
    "    # reset.\n",
    "    directory_plots = \"PPO_plots\"\n",
    "    if not os.path.exists(directory_plots):\n",
    "          os.makedirs(directory_plots)\n",
    "            \n",
    "    writer = SummaryWriter(log_dir = directory_plots )\n",
    "    max_ep_len = 300\n",
    "    # The training phase will sample and update for 1 million timestep.\n",
    "    max_training_steps = int(1e6)\n",
    "\n",
    "    # In order, to check ongoing progress, average reward is printed at every 10_000 timesteps.\n",
    "    print_freq = 10_000\n",
    "    \n",
    "    # Saving model parameters at every 1_00_000 timesteps.\n",
    "    save_model_freq = int(1e5)\n",
    "\n",
    "    action_std = 0.6                                    # Initial standard deviation.\n",
    "    action_std_decay_rate = 0.1                       # Decay rate of standard deviation.\n",
    "    min_action_std = 0.1                                # Threshold standard deviation.\n",
    "    action_std_decay_freq = int(2e5)                    # Decay the standard deviation every 2_00_000 timesteps\n",
    "\n",
    "    update_timestep = 1000                              # set old_policy parameters to new_policy parameters.\n",
    "    K_epochs = 100                                      # Number of epochs before updating old policy parameters.\n",
    "    eps_clip = 0.2                                      # clip range for surrogate loss function.\n",
    "    gamma = 0.99                                        # Discount factor.\n",
    "\n",
    "    lr_actor = 3e-3                                     # Learning rate for optimizer of actor network.\n",
    "    lr_critic = 0.001                                   # Learning rate for optimizer of critic network.\n",
    "    env_name = 'DoubleInvPendulum'\n",
    "    print(\"Training Environment:\" + env_name)\n",
    "    env = DoublePendulumEnv(init_state = state, dt = 0.02, max_initial_angle = 0)\n",
    "\n",
    "    observation_shape = 6  # Observation shape\n",
    "    action_shape = 1          # Action shape\n",
    "\n",
    "    # Creating a directory to store the model parameters during and after training.\n",
    "    directory = \"PPO2_Trained\"\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/'\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "    \n",
    "    checkpoint_path = directory + \"PPO2_{}.pth\".format(env_name)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"max training timesteps : \", max_training_steps)\n",
    "    print(\"max timesteps per episode : \", max_ep_len)\n",
    "    print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "    print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"state space dimension : \", observation_shape)\n",
    "    print(\"action space dimension : \", action_shape)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "    print(\"PPO K epochs : \", K_epochs)\n",
    "    print(\"PPO epsilon clip : \", eps_clip)\n",
    "    print(\"discount factor (gamma) : \", gamma)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"optimizer learning rate actor : \", lr_actor)\n",
    "    print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)\n",
    "\n",
    "    print(\"Starting the Training\")\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # To keep track of the progress\n",
    "    print_running_reward = 0    \n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    counter = 0\n",
    "\n",
    "    plot_episode = []\n",
    "    plot_reward = []\n",
    "\n",
    "    while time_step <= max_training_steps:\n",
    "        obs = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action = agent.select_action(obs)                           # Get action under old_policy given state.\n",
    "            action = unscaled_action(action)                            # Unscale the action.\n",
    "            obs, reward, done, _ = env.step(action)                     # Apply the action to environment.\n",
    "         \n",
    "\n",
    "            # Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)                             \n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                # Perform updates using sampled data.\n",
    "                agent.update()\n",
    "\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                # Decay standard deviation by 0.1.\n",
    "                agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward during 10_000 timesteps\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "\n",
    "                # Save the model parameters for test phase and tracking performance.\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Saving model at : \" + checkpoint_path)\n",
    "                agent.save(checkpoint_path)\n",
    "                print(\"Model saved\")\n",
    "                counter += 1\n",
    "                inter_checkpoint = directory + \"PPO_{}_{}00K.pth\".format(env_name, counter)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")               \n",
    "                print(\"Model parameters to check for intermediate performance saving:.\")\n",
    "                print(\"saving model at : \" + inter_checkpoint)\n",
    "                agent.save(inter_checkpoint)\n",
    "                if counter == 10:\n",
    "                    print(f\"Intermediate model saved for {counter}M\")   \n",
    "                else:\n",
    "                    print(f\"Intermediate model saved for {counter}00K\")                \n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        \n",
    "        # plot reward per 10 episode.\n",
    "        if i_episode % 10 == 0:\n",
    "            writer.add_scalar(\"Episode reward\", current_ep_reward,i_episode)\n",
    "            plot_episode.append(i_episode)\n",
    "            plot_reward.append(current_ep_reward)\n",
    "            \n",
    "\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Training Finished\")\n",
    "\n",
    "    \n",
    "    return plot_episode, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9ea05e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Training Environment:DoubleInvPendulum\n",
      "Environment initialized\n",
      "save checkpoint path : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  1000000\n",
      "max timesteps per episode :  300\n",
      "model saving frequency : 100000 timesteps\n",
      "printing average reward over episodes in last : 10000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  6\n",
      "action space dimension :  1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.6\n",
      "decay rate of std of action distribution :  0.1\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 200000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 1000 timesteps\n",
      "PPO K epochs :  100\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Starting the Training\n",
      "============================================================================================\n",
      "Episode : 38 \t\t Timestep : 10000 \t\t Average Reward : -364.78\n",
      "Episode : 94 \t\t Timestep : 20000 \t\t Average Reward : -179.81\n",
      "Episode : 165 \t\t Timestep : 30000 \t\t Average Reward : -78.86\n",
      "Episode : 249 \t\t Timestep : 40000 \t\t Average Reward : -27.09\n",
      "Episode : 331 \t\t Timestep : 50000 \t\t Average Reward : -10.67\n",
      "Episode : 410 \t\t Timestep : 60000 \t\t Average Reward : -37.43\n",
      "Episode : 496 \t\t Timestep : 70000 \t\t Average Reward : -59.75\n",
      "Episode : 568 \t\t Timestep : 80000 \t\t Average Reward : -126.31\n",
      "Episode : 626 \t\t Timestep : 90000 \t\t Average Reward : -180.38\n",
      "Episode : 679 \t\t Timestep : 100000 \t\t Average Reward : -211.33\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_100K.pth\n",
      "Intermediate model saved for 100K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 735 \t\t Timestep : 110000 \t\t Average Reward : -185.84\n",
      "Episode : 789 \t\t Timestep : 120000 \t\t Average Reward : -181.35\n",
      "Episode : 837 \t\t Timestep : 130000 \t\t Average Reward : -224.13\n",
      "Episode : 884 \t\t Timestep : 140000 \t\t Average Reward : -228.45\n",
      "Episode : 929 \t\t Timestep : 150000 \t\t Average Reward : -273.3\n",
      "Episode : 980 \t\t Timestep : 160000 \t\t Average Reward : -226.07\n",
      "Episode : 1029 \t\t Timestep : 170000 \t\t Average Reward : -237.77\n",
      "Episode : 1081 \t\t Timestep : 180000 \t\t Average Reward : -238.56\n",
      "Episode : 1133 \t\t Timestep : 190000 \t\t Average Reward : -220.51\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.5\n",
      "Episode : 1200 \t\t Timestep : 200000 \t\t Average Reward : -143.02\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_200K.pth\n",
      "Intermediate model saved for 200K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 1282 \t\t Timestep : 210000 \t\t Average Reward : -101.55\n",
      "Episode : 1362 \t\t Timestep : 220000 \t\t Average Reward : -100.35\n",
      "Episode : 1445 \t\t Timestep : 230000 \t\t Average Reward : -95.81\n",
      "Episode : 1539 \t\t Timestep : 240000 \t\t Average Reward : -62.66\n",
      "Episode : 1632 \t\t Timestep : 250000 \t\t Average Reward : -61.23\n",
      "Episode : 1712 \t\t Timestep : 260000 \t\t Average Reward : -100.78\n",
      "Episode : 1768 \t\t Timestep : 270000 \t\t Average Reward : -184.74\n",
      "Episode : 1820 \t\t Timestep : 280000 \t\t Average Reward : -197.64\n",
      "Episode : 1869 \t\t Timestep : 290000 \t\t Average Reward : -224.41\n",
      "Episode : 1916 \t\t Timestep : 300000 \t\t Average Reward : -257.7\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_300K.pth\n",
      "Intermediate model saved for 300K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 1971 \t\t Timestep : 310000 \t\t Average Reward : -165.02\n",
      "Episode : 2032 \t\t Timestep : 320000 \t\t Average Reward : -156.97\n",
      "Episode : 2092 \t\t Timestep : 330000 \t\t Average Reward : -157.92\n",
      "Episode : 2147 \t\t Timestep : 340000 \t\t Average Reward : -143.24\n",
      "Episode : 2194 \t\t Timestep : 350000 \t\t Average Reward : -220.56\n",
      "Episode : 2238 \t\t Timestep : 360000 \t\t Average Reward : -264.91\n",
      "Episode : 2286 \t\t Timestep : 370000 \t\t Average Reward : -239.52\n",
      "Episode : 2352 \t\t Timestep : 380000 \t\t Average Reward : -144.76\n",
      "Episode : 2404 \t\t Timestep : 390000 \t\t Average Reward : -246.74\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.4\n",
      "Episode : 2454 \t\t Timestep : 400000 \t\t Average Reward : -261.24\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_400K.pth\n",
      "Intermediate model saved for 400K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 2512 \t\t Timestep : 410000 \t\t Average Reward : -178.1\n",
      "Episode : 2568 \t\t Timestep : 420000 \t\t Average Reward : -208.55\n",
      "Episode : 2632 \t\t Timestep : 430000 \t\t Average Reward : -158.66\n",
      "Episode : 2689 \t\t Timestep : 440000 \t\t Average Reward : -181.87\n",
      "Episode : 2739 \t\t Timestep : 450000 \t\t Average Reward : -215.49\n",
      "Episode : 2783 \t\t Timestep : 460000 \t\t Average Reward : -266.98\n",
      "Episode : 2823 \t\t Timestep : 470000 \t\t Average Reward : -305.24\n",
      "Episode : 2865 \t\t Timestep : 480000 \t\t Average Reward : -298.92\n",
      "Episode : 2908 \t\t Timestep : 490000 \t\t Average Reward : -281.85\n",
      "Episode : 2972 \t\t Timestep : 500000 \t\t Average Reward : -165.32\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_500K.pth\n",
      "Intermediate model saved for 500K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 3040 \t\t Timestep : 510000 \t\t Average Reward : -139.97\n",
      "Episode : 3110 \t\t Timestep : 520000 \t\t Average Reward : -142.37\n",
      "Episode : 3183 \t\t Timestep : 530000 \t\t Average Reward : -133.57\n",
      "Episode : 3253 \t\t Timestep : 540000 \t\t Average Reward : -137.48\n",
      "Episode : 3321 \t\t Timestep : 550000 \t\t Average Reward : -134.54\n",
      "Episode : 3382 \t\t Timestep : 560000 \t\t Average Reward : -153.33\n",
      "Episode : 3441 \t\t Timestep : 570000 \t\t Average Reward : -162.52\n",
      "Episode : 3505 \t\t Timestep : 580000 \t\t Average Reward : -150.7\n",
      "Episode : 3550 \t\t Timestep : 590000 \t\t Average Reward : -248.25\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.3\n",
      "Episode : 3619 \t\t Timestep : 600000 \t\t Average Reward : -129.69\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_600K.pth\n",
      "Intermediate model saved for 600K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 3689 \t\t Timestep : 610000 \t\t Average Reward : -124.26\n",
      "Episode : 3741 \t\t Timestep : 620000 \t\t Average Reward : -191.86\n",
      "Episode : 3791 \t\t Timestep : 630000 \t\t Average Reward : -212.2\n",
      "Episode : 3835 \t\t Timestep : 640000 \t\t Average Reward : -248.11\n",
      "Episode : 3875 \t\t Timestep : 650000 \t\t Average Reward : -291.8\n",
      "Episode : 3943 \t\t Timestep : 660000 \t\t Average Reward : -136.96\n",
      "Episode : 4007 \t\t Timestep : 670000 \t\t Average Reward : -123.72\n",
      "Episode : 4075 \t\t Timestep : 680000 \t\t Average Reward : -114.91\n",
      "Episode : 4140 \t\t Timestep : 690000 \t\t Average Reward : -118.29\n",
      "Episode : 4204 \t\t Timestep : 700000 \t\t Average Reward : -130.96\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_700K.pth\n",
      "Intermediate model saved for 700K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 4266 \t\t Timestep : 710000 \t\t Average Reward : -142.61\n",
      "Episode : 4335 \t\t Timestep : 720000 \t\t Average Reward : -112.41\n",
      "Episode : 4404 \t\t Timestep : 730000 \t\t Average Reward : -109.86\n",
      "Episode : 4480 \t\t Timestep : 740000 \t\t Average Reward : -96.03\n",
      "Episode : 4543 \t\t Timestep : 750000 \t\t Average Reward : -131.89\n",
      "Episode : 4594 \t\t Timestep : 760000 \t\t Average Reward : -180.94\n",
      "Episode : 4653 \t\t Timestep : 770000 \t\t Average Reward : -139.87\n",
      "Episode : 4705 \t\t Timestep : 780000 \t\t Average Reward : -172.4\n",
      "Episode : 4765 \t\t Timestep : 790000 \t\t Average Reward : -141.69\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to :  0.2\n",
      "Episode : 4828 \t\t Timestep : 800000 \t\t Average Reward : -130.89\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_800K.pth\n",
      "Intermediate model saved for 800K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 4890 \t\t Timestep : 810000 \t\t Average Reward : -132.41\n",
      "Episode : 4956 \t\t Timestep : 820000 \t\t Average Reward : -121.44\n",
      "Episode : 5023 \t\t Timestep : 830000 \t\t Average Reward : -132.78\n",
      "Episode : 5067 \t\t Timestep : 840000 \t\t Average Reward : -248.19\n",
      "Episode : 5124 \t\t Timestep : 850000 \t\t Average Reward : -155.8\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_episode, reward_episode = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32a379",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='Directory_plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fa2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Reward per every 10 episodes.\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(n_episode, reward_episode)\n",
    "ax.set_title('Total number of episodes vs rewards per episode', fontsize=20)\n",
    "ax.set_xlabel('Episode', fontsize=20)\n",
    "ax.set_ylabel('Reward', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9178bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "state = state0\n",
    "\n",
    "dt = 0.1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                     xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "energy_text = ax.text(0.02, 0.90, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    #time_text.set_text('')\n",
    "    energy_text.set_text('')\n",
    "    return line\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"perform animation step\"\"\"\n",
    "    global state, dt\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    u = agent.select_action(state_t)\n",
    "    action\n",
    "    state = get_next_state(state,u[0].detach().numpy(),dt)\n",
    "    XY = state_to_coords(state)\n",
    "    en = get_energy(state)\n",
    "    \n",
    "    line.set_data(XY[0],XY[1])\n",
    "    energy_text.set_text(f'energy = {en}')\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=100,\n",
    "                             interval=10, blit=True, init_func=init)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9edf45",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
