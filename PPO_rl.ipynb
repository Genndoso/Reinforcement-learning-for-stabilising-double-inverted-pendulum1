{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ddd44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Dynamics import get_next_state, state_to_coords, get_energy\n",
    "from Environment import DoublePendulumEnv, normalize_angle\n",
    "from PPO.Proximal_Policy_Optimization import PPO, unscaled_action\n",
    "from PPO.train import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad651c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d37f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98b69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi/2,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state\n",
    "#max_initial_angle = 3 * 2 * np.pi / 360\n",
    "max_initial_angle = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455d941",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8b460c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePendulumEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, init_state, dt=0.02, max_initial_angle = 3 * 2 * np.pi / 360):\n",
    "        self.action_space = Box(low=-500, high=500)\n",
    "        self.observation_space = 6\n",
    "        self.state = init_state\n",
    "        self.init_state = init_state\n",
    "        self.dt = dt\n",
    "        print('Environment initialized')\n",
    "        self.init_coords = state_to_coords(init_state)\n",
    "        self.max_initial_angle = max_initial_angle\n",
    "    def _take_action(self, action):\n",
    "        self.state = get_next_state(self.state, action, self.dt)\n",
    "\n",
    "    def _reward_function(self, done):\n",
    "        \"\"\"\n",
    "\n",
    "        # Reward system 1\n",
    "        Check whether 1 and 2 cart pole are in angle range between 80 and 100 degrees\n",
    "        agent will agent a reward in range [0, 1]\n",
    "\n",
    "        else:\n",
    "            If angle of pole 1 and 2  are greater than 10 degrees, therefore, it makes sense\n",
    "            to terminate the environment and reset/restart.\n",
    "            agent will get a  reward = -1\n",
    "\n",
    "        # Reward system 2\n",
    "        If cart is in given range of x = [-5, 5] then agent will get a reward 0.5 every steps.\n",
    "        Otherwise, it penalies the system heavily of a penalty = -50 and system is done here.\n",
    "\n",
    "        # Reward system 3\n",
    "        # this is unused. This an analog to reward system 1 but for coordinates\n",
    "        If cart pole is not in the same line with cart then it will give additional penalty\n",
    "\n",
    "\n",
    "        # Reward system 4\n",
    "        Velocity penalty (halves the reward if spinning too fast)\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        reward = 0\n",
    "        # degree reward\n",
    "        normalized_angle_1 = np.degrees(normalize_angle(state[1]))\n",
    "        normalized_angle_2 = np.degrees(normalize_angle(state[2]))\n",
    "\n",
    "        if normalized_angle_1 > 87 and normalized_angle_1 < 93:\n",
    "            reward = 1 - (90 - normalized_angle_1) * 0.01\n",
    "            if normalized_angle_2 > 80 and normalized_angle_2 < 100:\n",
    "                reward += reward + 1 - (90 - normalized_angle_2) * 0.01\n",
    "            reward *= 2\n",
    "       \n",
    "        else: \n",
    "            reward = -10\n",
    "            done = True\n",
    "\n",
    "       # another degree reward system\n",
    "        # cost = 2*(normalize_angle(state[1]) - np.pi/2) + \\\n",
    "        #                2*(normalize_angle(state[2]) - np.pi/2)\n",
    "\n",
    "        # reward = -np.abs(cost)\n",
    "\n",
    "        # another degree_reward system\n",
    "\n",
    "        #         deg_reward = ((np.sin(state[1]))*10 + (np.sin(state[2]))*10)/2\n",
    "        #         #if np.sin(state[1]\n",
    "        #         reward += deg_reward\n",
    "        #         print(state[1])\n",
    "\n",
    "        # distance penalty\n",
    "        if state[0] < 2 and state[0] > -2:\n",
    "            pass\n",
    "        else:\n",
    "            reward -= -50\n",
    "            done = True\n",
    "\n",
    "       # distance2 rew\n",
    "        state_coords = state_to_coords(state)\n",
    "               # dist_pen = (state_coords[0][1] - state_coords[0][0])**2 +  (state_coords[0][2] - state_coords[0][0])**2\n",
    "        dist_rew =  -( state_coords[1][1] - self.init_coords[1][1]) -  ( state_coords[1][2] - self.init_coords[1][2])*2\n",
    "        reward -= dist_rew\n",
    "        \n",
    "\n",
    "       # velocity penalty\n",
    "        # vel_pen = ((1 + np.exp(-0.5 * state[-3:] ** 2)) / 2).sum()/10\n",
    "        # reward -= vel_pen\n",
    "        # print(-vel_pen)\n",
    "\n",
    "        return reward, done\n",
    "\n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        observation -  [x,phi,theta,dx,dphi,dtheta]\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -5 m                  5 m\n",
    "        1       Pole1 Angle               -pi                     +pi\n",
    "        2       Pole2 Angle               -pi                     +pi\n",
    "        3       Cart Velocity             -Inf                    Inf\n",
    "        4       Pole1 Angular Velocity    -Inf                    Inf\n",
    "        5       Pole1 Angular Velocity    -Inf                    Inf\n",
    "\n",
    "        \"\"\"\n",
    "        done = False\n",
    "        info = {}\n",
    "        self._take_action(action)\n",
    "\n",
    "       \n",
    "        reward, done = self._reward_function(done)\n",
    "        return np.array(self.state), reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Compute the render frames as specified by render_mode attribute during initialization of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        ani = animation.FuncAnimation(fig, animate, frames=300,\n",
    "                                      interval=20, blit=True, init_func=init)\n",
    "        plt.show()\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to an initial state and returns the initial observation.\n",
    "        \"\"\"\n",
    "        self.rew_sum = 0\n",
    "        self.state = self.init_state\n",
    "        d = np.random.uniform(-self.max_initial_angle, self.max_initial_angle)\n",
    "        self.state[1] = np.pi/2 + np.random.uniform(-self.max_initial_angle, self.max_initial_angle)\n",
    "        self.state[2] = np.pi/2 + np.random.uniform(-self.max_initial_angle, self.max_initial_angle)\n",
    "\n",
    "        return np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe53bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    print(\"============================================================================================\")\n",
    "    # max. timestep per episode. For DoubleCartPoleEnv, time constraint is 200 timesteps. After that environment is \n",
    "    # reset.\n",
    "    directory_plots = \"PPO_plots\"\n",
    "    if not os.path.exists(directory_plots):\n",
    "          os.makedirs(directory_plots)\n",
    "            \n",
    "    writer = SummaryWriter(log_dir = directory_plots )\n",
    "    max_ep_len = 400\n",
    "    # The training phase will sample and update for 1 million timestep.\n",
    "    max_training_steps = int(1e6)\n",
    "\n",
    "    # In order, to check ongoing progress, average reward is printed at every 10_000 timesteps.\n",
    "    print_freq = 10_000\n",
    "    \n",
    "    # Saving model parameters at every 1_00_000 timesteps.\n",
    "    save_model_freq = int(1e5)\n",
    "\n",
    "    action_std = 0.2                                    # Initial standard deviation.\n",
    "    action_std_decay_rate = 0.1                       # Decay rate of standard deviation.\n",
    "    min_action_std = 0.1                                # Threshold standard deviation.\n",
    "    action_std_decay_freq = int(2e5)                    # Decay the standard deviation every 2_00_000 timesteps\n",
    "\n",
    "    update_timestep = 2000                              # set old_policy parameters to new_policy parameters.\n",
    "    K_epochs = 100                                      # Number of epochs before updating old policy parameters.\n",
    "    eps_clip = 0.2                                      # clip range for surrogate loss function.\n",
    "    gamma = 0.99                                        # Discount factor.\n",
    "\n",
    "    lr_actor = 3e-3                                   # Learning rate for optimizer of actor network.\n",
    "    lr_critic = 0.001                                  # Learning rate for optimizer of critic network.\n",
    "    env_name = 'DoubleInvPendulum'\n",
    "    print(\"Training Environment:\" + env_name)\n",
    "    env = DoublePendulumEnv(init_state = state, dt = 0.02)\n",
    "\n",
    "    observation_shape = 6  # Observation shape\n",
    "    action_shape = 1          # Action shape\n",
    "\n",
    "    # Creating a directory to store the model parameters during and after training.\n",
    "    directory = \"PPO2_Trained\"\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/'\n",
    "    if not os.path.exists(directory):\n",
    "          os.makedirs(directory)\n",
    "    \n",
    "    checkpoint_path = directory + \"PPO2_{}.pth\".format(env_name)\n",
    "    print(\"save checkpoint path : \" + checkpoint_path)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"max training timesteps : \", max_training_steps)\n",
    "    print(\"max timesteps per episode : \", max_ep_len)\n",
    "    print(\"model saving frequency : \" + str(save_model_freq) + \" timesteps\")\n",
    "    print(\"printing average reward over episodes in last : \" + str(print_freq) + \" timesteps\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"state space dimension : \", observation_shape)\n",
    "    print(\"action space dimension : \", action_shape)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"Initializing a continuous action space policy\")\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"starting std of action distribution : \", action_std)\n",
    "    print(\"decay rate of std of action distribution : \", action_std_decay_rate)\n",
    "    print(\"minimum std of action distribution : \", min_action_std)\n",
    "    print(\"decay frequency of std of action distribution : \" + str(action_std_decay_freq) + \" timesteps\")\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "    print(\"PPO K epochs : \", K_epochs)\n",
    "    print(\"PPO epsilon clip : \", eps_clip)\n",
    "    print(\"discount factor (gamma) : \", gamma)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"optimizer learning rate actor : \", lr_actor)\n",
    "    print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)\n",
    "\n",
    "    print(\"Starting the Training\")\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # To keep track of the progress\n",
    "    print_running_reward = 0    \n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    counter = 0\n",
    "\n",
    "    plot_episode = []\n",
    "    plot_reward = []\n",
    "\n",
    "    while time_step <= max_training_steps:\n",
    "        obs = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            action = agent.select_action(obs)                           # Get action under old_policy given state.\n",
    "            action = unscaled_action(action)                            # Unscale the action.\n",
    "            obs, reward, done, _ = env.step(action)                     # Apply the action to environment.\n",
    "         \n",
    "\n",
    "            # Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)                             \n",
    "\n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                # Perform updates using sampled data.\n",
    "                agent.update()\n",
    "\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                # Decay standard deviation by 0.1.\n",
    "                agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward during 10_000 timesteps\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step, print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "\n",
    "                # Save the model parameters for test phase and tracking performance.\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Saving model at : \" + checkpoint_path)\n",
    "                agent.save(checkpoint_path)\n",
    "                print(\"Model saved\")\n",
    "                counter += 1\n",
    "                inter_checkpoint = directory + \"PPO_{}_{}00K.pth\".format(env_name, counter)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")               \n",
    "                print(\"Model parameters to check for intermediate performance saving:.\")\n",
    "                print(\"saving model at : \" + inter_checkpoint)\n",
    "                agent.save(inter_checkpoint)\n",
    "                if counter == 10:\n",
    "                    print(f\"Intermediate model saved for {counter}M\")   \n",
    "                else:\n",
    "                    print(f\"Intermediate model saved for {counter}00K\")                \n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                \n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "        \n",
    "        # plot reward per 10 episode.\n",
    "        if i_episode % 10 == 0:\n",
    "            writer.add_scalar(\"Episode reward\", current_ep_reward,i_episode)\n",
    "            plot_episode.append(i_episode)\n",
    "            plot_reward.append(current_ep_reward)\n",
    "            \n",
    "\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "    print(\"Training Finished\")\n",
    "\n",
    "    \n",
    "    return plot_episode, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8beec190",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Training Environment:DoubleInvPendulum\n",
      "Environment initialized\n",
      "save checkpoint path : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "--------------------------------------------------------------------------------------------\n",
      "max training timesteps :  1000000\n",
      "max timesteps per episode :  400\n",
      "model saving frequency : 100000 timesteps\n",
      "printing average reward over episodes in last : 10000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "state space dimension :  6\n",
      "action space dimension :  1\n",
      "--------------------------------------------------------------------------------------------\n",
      "Initializing a continuous action space policy\n",
      "--------------------------------------------------------------------------------------------\n",
      "starting std of action distribution :  0.2\n",
      "decay rate of std of action distribution :  0.1\n",
      "minimum std of action distribution :  0.1\n",
      "decay frequency of std of action distribution : 200000 timesteps\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 2000 timesteps\n",
      "PPO K epochs :  100\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "Starting the Training\n",
      "============================================================================================\n",
      "Episode : 2619 \t\t Timestep : 10000 \t\t Average Reward : 6.91\n",
      "Episode : 4177 \t\t Timestep : 20000 \t\t Average Reward : 22.35\n",
      "Episode : 5274 \t\t Timestep : 30000 \t\t Average Reward : 38.45\n",
      "Episode : 6127 \t\t Timestep : 40000 \t\t Average Reward : 53.68\n",
      "Episode : 6976 \t\t Timestep : 50000 \t\t Average Reward : 54.03\n",
      "Episode : 7792 \t\t Timestep : 60000 \t\t Average Reward : 56.25\n",
      "Episode : 8692 \t\t Timestep : 70000 \t\t Average Reward : 50.03\n",
      "Episode : 9544 \t\t Timestep : 80000 \t\t Average Reward : 54.1\n",
      "Episode : 10424 \t\t Timestep : 90000 \t\t Average Reward : 51.56\n",
      "Episode : 11283 \t\t Timestep : 100000 \t\t Average Reward : 53.15\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_100K.pth\n",
      "Intermediate model saved for 100K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 12205 \t\t Timestep : 110000 \t\t Average Reward : 48.58\n",
      "Episode : 13086 \t\t Timestep : 120000 \t\t Average Reward : 51.51\n",
      "Episode : 14046 \t\t Timestep : 130000 \t\t Average Reward : 46.15\n",
      "Episode : 14977 \t\t Timestep : 140000 \t\t Average Reward : 48.27\n",
      "Episode : 15904 \t\t Timestep : 150000 \t\t Average Reward : 48.43\n",
      "Episode : 16883 \t\t Timestep : 160000 \t\t Average Reward : 45.05\n",
      "Episode : 17867 \t\t Timestep : 170000 \t\t Average Reward : 44.83\n",
      "Episode : 18832 \t\t Timestep : 180000 \t\t Average Reward : 45.96\n",
      "Episode : 19857 \t\t Timestep : 190000 \t\t Average Reward : 42.35\n",
      "--------------------------------------------------------------------------------------------\n",
      "setting actor output action_std to min_action_std :  0.1\n",
      "Episode : 20859 \t\t Timestep : 200000 \t\t Average Reward : 43.71\n",
      "--------------------------------------------------------------------------------------------\n",
      "Saving model at : PPO2_Trained/PPO2_DoubleInvPendulum.pth\n",
      "Model saved\n",
      "--------------------------------------------------------------------------------------------\n",
      "Model parameters to check for intermediate performance saving:.\n",
      "saving model at : PPO2_Trained/PPO_DoubleInvPendulum_200K.pth\n",
      "Intermediate model saved for 200K\n",
      "--------------------------------------------------------------------------------------------\n",
      "Episode : 21401 \t\t Timestep : 210000 \t\t Average Reward : 93.04\n",
      "Episode : 21887 \t\t Timestep : 220000 \t\t Average Reward : 104.57\n",
      "Episode : 22353 \t\t Timestep : 230000 \t\t Average Reward : 110.06\n",
      "Episode : 22777 \t\t Timestep : 240000 \t\t Average Reward : 120.85\n"
     ]
    }
   ],
   "source": [
    "\n",
    "n_episode, reward_episode = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir='PPO_plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Reward per every 10 episodes.\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(n_episode, reward_episode)\n",
    "ax.set_title('Total number of episodes vs rewards per episode', fontsize=20)\n",
    "ax.set_xlabel('Episode', fontsize=20)\n",
    "ax.set_ylabel('Reward', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "state = state0\n",
    "\n",
    "dt = 0.1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                     xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "energy_text = ax.text(0.02, 0.90, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    #time_text.set_text('')\n",
    "    energy_text.set_text('')\n",
    "    return line\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"perform animation step\"\"\"\n",
    "    global state, dt\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    u = agent.select_action(state_t)\n",
    "    action\n",
    "    state = get_next_state(state,u[0].detach().numpy(),dt)\n",
    "    XY = state_to_coords(state)\n",
    "    en = get_energy(state)\n",
    "    \n",
    "    line.set_data(XY[0],XY[1])\n",
    "    energy_text.set_text(f'energy = {en}')\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=100,\n",
    "                             interval=10, blit=True, init_func=init)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466af0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
