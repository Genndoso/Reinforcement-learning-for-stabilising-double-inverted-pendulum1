{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d27ee8d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from casadi import *\n",
    "%matplotlib widget\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "\n",
    "t = SX.sym('t')\n",
    "l = SX.sym('l',2)\n",
    "m = SX.sym('m',3)\n",
    "phi = SX.sym('phi',3)\n",
    "dphi = SX.sym('dphi',3)\n",
    "\n",
    "G = SX.sym('g')\n",
    "g = vcat([0,G])\n",
    "\n",
    "u = SX.sym('u')\n",
    "f = vcat([u,0,0])\n",
    "\n",
    "P1 = vcat([phi[0],0])\n",
    "P2 = vcat([l[0] * cos(phi[1]), l[0] * sin(phi[1])]) + P1\n",
    "P3 = vcat([l[1] * cos(phi[2]), l[1] * sin(phi[2])]) + P2\n",
    "P = horzcat(P1,P2,P3)\n",
    "\n",
    "J1 = jacobian(P1,phi)\n",
    "J2 = jacobian(P2,phi)\n",
    "J3 = jacobian(P3,phi)\n",
    "J = [J1,J2,J3]\n",
    "\n",
    "T = SX([0])\n",
    "U = SX([0])\n",
    "for i in range(3):\n",
    "    T += m[i]/2 *(J[i] @ dphi).T @ J[i] @ dphi \n",
    "    U += m[i] * g.T @ P[:,i]\n",
    "\n",
    "def diff_time(J,x,dx):\n",
    "    J_dot = []\n",
    "    for vect in horzsplit(J):\n",
    "        J_dot.append(jacobian(vect,x) @ dx)\n",
    "    J_dot = hcat(J_dot)\n",
    "    return J_dot\n",
    "\n",
    "J1_dot = diff_time(J1,phi,dphi)\n",
    "J2_dot = diff_time(J2,phi,dphi)\n",
    "J3_dot = diff_time(J3,phi,dphi)\n",
    "J_dot = [J1_dot,J2_dot,J3_dot]\n",
    "\n",
    "A = SX(3,3)\n",
    "B = SX(3,3)\n",
    "C = SX(3,3)\n",
    "D = SX(3,2)\n",
    "for i in range(3):\n",
    "    A += m[i] * (J_dot[i].T @ J[i] + J[i].T @ J_dot[i])\n",
    "    B += m[i] * (J[i].T @ J[i])\n",
    "    C += m[i] * (J_dot[i].T @ J[i])\n",
    "    D += m[i] * (J[i].T)\n",
    "rhs = solve(B,(f + (C - A) @ dphi - D @ g))\n",
    "rhs = vertcat(dphi,rhs)\n",
    "\n",
    "m_real = SX([1,1,1])\n",
    "l_real = SX([1,1])\n",
    "G_real = SX([10])\n",
    "\n",
    "my_rhs = substitute([rhs],[m,l,G],[m_real,l_real,G_real])[0]\n",
    "my_rhs = Function('rhs',[phi,dphi,u],[my_rhs])\n",
    "\n",
    "energy = substitute([T + U],[m,l,G],[m_real,l_real,G_real])[0]\n",
    "energy = Function('energy',[phi,dphi],[energy])\n",
    "\n",
    "my_P = substitute([P],[m,l],[m_real,l_real])\n",
    "my_P = Function('P',[phi],my_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc84c9b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "P1 = vcat([phi[0],0])\n",
    "P2 = vcat([l[0] * cos(phi[1]), l[0] * sin(phi[1])]) + P1\n",
    "P3 = vcat([l[1] * cos(phi[2]), l[1] * sin(phi[2])]) + P2\n",
    "P = horzcat(P1,P2,P3)\n",
    "\n",
    "J1 = jacobian(P1,phi)\n",
    "J2 = jacobian(P2,phi)\n",
    "J3 = jacobian(P3,phi)\n",
    "J = [J1,J2,J3]\n",
    "\n",
    "T = SX([0])\n",
    "U = SX([0])\n",
    "for i in range(3):\n",
    "    T += m[i]/2 *(J[i] @ dphi).T @ J[i] @ dphi \n",
    "    U += m[i] * g.T @ P[:,i]\n",
    "\n",
    "def diff_time(J,x,dx):\n",
    "    J_dot = []\n",
    "    for vect in horzsplit(J):\n",
    "        J_dot.append(jacobian(vect,x) @ dx)\n",
    "    J_dot = hcat(J_dot)\n",
    "    return J_dot\n",
    "\n",
    "J1_dot = diff_time(J1,phi,dphi)\n",
    "J2_dot = diff_time(J2,phi,dphi)\n",
    "J3_dot = diff_time(J3,phi,dphi)\n",
    "J_dot = [J1_dot,J2_dot,J3_dot]\n",
    "\n",
    "A = SX(3,3)\n",
    "B = SX(3,3)\n",
    "C = SX(3,3)\n",
    "D = SX(3,2)\n",
    "for i in range(3):\n",
    "    A += m[i] * (J_dot[i].T @ J[i] + J[i].T @ J_dot[i])\n",
    "    B += m[i] * (J[i].T @ J[i])\n",
    "    C += m[i] * (J_dot[i].T @ J[i])\n",
    "    D += m[i] * (J[i].T)\n",
    "rhs = solve(B,(f + (C - A) @ dphi - D @ g))\n",
    "rhs = vertcat(dphi,rhs)\n",
    "\n",
    "m_real = SX([1,1,1])\n",
    "l_real = SX([1,1])\n",
    "G_real = SX([10])\n",
    "\n",
    "my_rhs = substitute([rhs],[m,l,G],[m_real,l_real,G_real])[0]\n",
    "my_rhs = Function('rhs',[phi,dphi,u],[my_rhs])\n",
    "\n",
    "energy = substitute([T + U],[m,l,G],[m_real,l_real,G_real])[0]\n",
    "energy = Function('energy',[phi,dphi],[energy])\n",
    "\n",
    "my_P = substitute([P],[m,l],[m_real,l_real])\n",
    "my_P = Function('P',[phi],my_P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4aaaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import integrate\n",
    "def get_next_state(state,u,dt):\n",
    "    return integrate.odeint(lambda x,t: my_rhs.call([x[:3],x[3:],[u]])[0].T.full()[0] , state, [0,dt])[1]\n",
    "def state_to_coords(state):\n",
    "    return my_P.call([state[:3]])[0].full()\n",
    "def get_energy(state):\n",
    "     return energy.call([state[:3],state[3:]])[0].full()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0e3443c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "443a4413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.        , 1.57079633, 1.57079633, 0.        , 0.        ,\n",
       "       0.        ])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi/2,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e909707",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4aa285ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DoublePendulumEnv(gym.Env):\n",
    "    \n",
    "    def __init__(self, init_state, dt = 0.1):\n",
    "        self.action_space = Box(low = -2, high = 2)\n",
    "        self.observation_space = 6\n",
    "        self.state = init_state\n",
    "        self.init_state = init_state\n",
    "        self.dt = dt\n",
    "        print('Environment initialized')\n",
    "        self.init_coords = state_to_coords(init_state)\n",
    "\n",
    "    def _take_action(self,action):\n",
    "        self.state = get_next_state(self.state,action,self.dt)\n",
    "\n",
    "    def _reward_function(self, done):\n",
    "        \"\"\"\n",
    "        if Condition 1: Check if the absolute angle of pole 1 is less than 10 degrees.\n",
    "            ->> agent will get reward between [0, 0.5]\n",
    "            if Condition 2: Check if the absolute angle of pole 2 is less than 10 degrees.\n",
    "                ->> agent will get reward from previous condition plus another reward in range [0,0.5]\n",
    "                ->> therefore, at equilibrium, reward = 1.0\n",
    "        else:\n",
    "            absolute angle of pole 1 is greater than 10 degrees and the state space associated with this state of environment\n",
    "            is not worth exploring. Therefore, it makes sense to terminate the environment and reset/restart.\n",
    "            reward = -1\n",
    "\n",
    "        This means that agent will collect more reward trying to stay in arc of +10, -10 degrees. Every time, agent's\n",
    "        action leads to a state outside the desired area, agent will get negative reward and a direct termination.\n",
    "\n",
    "        Condition 3: If cart position is far away from the initial state then there will be a penalty\n",
    "\n",
    "        Condition 4: If cart pole is not in the same line with cart then it will give additional penalty\n",
    "        \n",
    "        Condition 5: velocity penalty (halves the reward if spinning too fast)\n",
    "    \n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        reward = 0\n",
    "        # degree reward\n",
    "        normalized_angle_1 = np.degrees(normalize_angle(state[1]))\n",
    "        normalized_angle_2 = np.degrees(normalize_angle(state[2]))\n",
    "\n",
    "#         if normalized_angle_2 > 80 and normalized_angle_2 < 110 :\n",
    "#             reward = 9 -  (90 - normalized_angle_2)*0.1\n",
    "# #             if np.abs(np.degrees(state[2])) < 100:\n",
    "# #                 reward = reward + 9 - (90 - np.degrees(state[2]))*0.1\n",
    "# #                 reward *= 1\n",
    "#         else:\n",
    "#             reward += -10\n",
    "#             done = True\n",
    "            \n",
    "        #another degree reward system\n",
    "        cost = 10*(normalized_angle_1 - np.pi) + \\\n",
    "               10*(normalized_angle_2 - np.pi)\n",
    "\n",
    "        reward = -cost\n",
    "\n",
    "# degree_reward for staying upright\n",
    "        \n",
    "        \n",
    "#         deg_reward = ((np.sin(state[1]))*10 + (np.sin(state[2]))*10)/2\n",
    "#         #if np.sin(state[1]\n",
    "#         reward += deg_reward\n",
    "#         print(state[1])\n",
    "                  \n",
    "        \n",
    "        # distance penalty\n",
    "        if state[0] < 2  and state[0] > -2:\n",
    "            reward += 5\n",
    "        else:\n",
    "            reward -= -50\n",
    "            done = True\n",
    "            \n",
    "        # distance2 rew\n",
    "#         state_coords = state_to_coords(state)\n",
    "#        # dist_pen = (state_coords[0][1] - state_coords[0][0])**2 +  (state_coords[0][2] - state_coords[0][0])**2\n",
    "#         dist_rew =  self.init_coords[1][1] + self.init_coords[1][2] + ( state_coords[1][1] - self.init_coords[1][1])*5 +  ( state_coords[1][2] - self.init_coords[1][2])*5\n",
    "#         reward += dist_pen\n",
    "        \n",
    "        \n",
    "     \n",
    "    \n",
    "        #velocity penalty\n",
    "        vel_pen = ((1 + np.exp(-0.5 * state[-3:] ** 2)) / 2).sum()/10\n",
    "        reward -= vel_pen\n",
    "       \n",
    "       \n",
    "        return reward, done\n",
    "        \n",
    "    def step(self, action):\n",
    "        \"\"\"\n",
    "        observation -  [x,phi,theta,dx,dphi,dtheta]\n",
    "        Num     Observation               Min                     Max\n",
    "        0       Cart Position             -4 m                  4 m\n",
    "        1       Pole1 Angle               -pi                     +pi\n",
    "        2       Pole2 Angle               -pi                     +pi\n",
    "        3       Cart Velocity             -Inf                    Inf\n",
    "        4       Pole1 Angular Velocity    -Inf                    Inf\n",
    "        5       Pole1 Angular Velocity    -Inf                    Inf\n",
    "        \n",
    "        \"\"\"\n",
    "        done = False\n",
    "        info = {}\n",
    "        self._take_action(action)\n",
    "        \n",
    "        #alive_rew = 10\n",
    "        reward, done = self._reward_function(done)\n",
    "        return np.array(self.state), reward, done, info\n",
    "\n",
    "    def render(self):\n",
    "        \"\"\"\n",
    "        Compute the render frames as specified by render_mode attribute during initialization of the environment.\n",
    "\n",
    "        \"\"\"\n",
    "        state = self.state\n",
    "        ani = animation.FuncAnimation(fig, animate, frames=300,\n",
    "                              interval=20, blit=True, init_func=init)\n",
    "        plt.show()\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Resets the environment to an initial state and returns the initial observation.\n",
    "        \"\"\"\n",
    "        self.rew_sum = 0\n",
    "        self.state = self.init_state\n",
    "        \n",
    "        return  np.array(self.state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "82a66416",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.position = 0\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append(None)\n",
    "        self.buffer[self.position] = (state, action, reward, next_state, done)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        state, action, reward, next_state, done = map(np.stack, zip(*batch))\n",
    "        return state, action, reward, next_state, done\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "047d0e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, action_std_init, hidden_size = 528):\n",
    "        super(ActorCritic, self).__init__()\n",
    "\n",
    "        self.action_dim = action_dim\n",
    "        # Instead of directly using variane as input to normal distribution, standard deviation is set as hyperparameter \n",
    "        # and used to calculate the variance.\n",
    "        self.action_var = torch.full((action_dim,), action_std_init * action_std_init)\n",
    "        # Actor NN\n",
    "        self.hidden_size = hidden_size\n",
    "        self.actor = nn.Sequential(\n",
    "            nn.Linear(state_dim, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_size, self.hidden_size),\n",
    "            nn.Tanh(),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(self.hidden_size, action_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        # Critic NN\n",
    "        self.critic = nn.Sequential(\n",
    "                        nn.Linear(state_dim, self.hidden_size),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Dropout(),\n",
    "                        nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Dropout(),\n",
    "                        nn.Linear(self.hidden_size, self.hidden_size),\n",
    "                        nn.Tanh(),\n",
    "                        nn.Dropout(),\n",
    "                        nn.Linear(self.hidden_size, 1)\n",
    "                    )\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        \"\"\"\n",
    "        Performance of PPO is sensitive to standard deviation. The standard deviation is decaying by 0.05 \n",
    "        every 90000 timestep. This function sets new standard deviation to be used while creating normal distribution.\n",
    "        \"\"\"\n",
    "        self.action_var = torch.full((self.action_dim,), new_action_std * new_action_std)\n",
    "\n",
    "    def get_action(self, state):\n",
    "        \"\"\"\n",
    "        Called during sampling phase.\n",
    "        Passing a State through actor network to get the mean and plot normal distribution based on that mean to sample\n",
    "        an action.\n",
    "        \"\"\"\n",
    "        action_mean = self.actor(state)                            # output of actor network\n",
    "        cov_mat = torch.diag(self.action_var).unsqueeze(dim=0)     # Variance of Normal distribution.\n",
    "        policy = MultivariateNormal(action_mean, cov_mat)          # Generating a policy based on (mean, variance)\n",
    "\n",
    "        action = policy.sample()                                   # Action sampeled from policy and to be applied.\n",
    "        action_logprob = policy.log_prob(action)                   # log of prob. of that action given the distribution.\n",
    "\n",
    "        # Since, we are only interested in the action and its prob., we do not perform SGD on them and can detach the \n",
    "        # computational graph associated with it.\n",
    "        return action.detach(), action_logprob.detach()             \n",
    "    \n",
    "\n",
    "    def evaluate(self, state, action):\n",
    "        \"\"\"\n",
    "        Called during update phase.\n",
    "        In order to calculate the ratio of prob. of taking an action given a state under new policy, we need to\n",
    "        pass the old sampled state and action taken old policy and get mean, value and logprob under new policy.\n",
    "        New policy means updated model weights compared to the weights using which the action and value was approximated\n",
    "        during sampling phase.\n",
    "        \"\"\"\n",
    "        action_mean = self.actor(state)\n",
    "\n",
    "        action_var = self.action_var.expand_as(action_mean)\n",
    "        cov_mat = torch.diag_embed(action_var)\n",
    "        policy = MultivariateNormal(action_mean, cov_mat)\n",
    "\n",
    "        if self.action_dim == 1:\n",
    "            action = action.reshape(-1, self.action_dim)\n",
    "        action_logprobs = policy.log_prob(action)\n",
    "        policy_entropy = policy.entropy()\n",
    "        state_values = self.critic(state)\n",
    "        # The 'action_logprobs' will be used to calculate the ratio for surrogate loss.\n",
    "        # The 'state_values' will be used to calculate the MSE for critic loss.\n",
    "        # The 'policy_entropy' will be used give bonus for exploration in final loss.\n",
    "        return action_logprobs, state_values, policy_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5625dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr_actor, lr_critic, gamma, K_epochs, eps_clip, action_std_init=0.6):\n",
    "        self.action_std = action_std_init\n",
    "\n",
    "        self.gamma = gamma                                  # Discount factor\n",
    "        self.eps_clip = eps_clip                            # Clipping range for Clipped Surrogate Function.\n",
    "        self.K_epochs = K_epochs                            # Total number of epochs.\n",
    "\n",
    "        self.buffer = ReplayBuffer()\n",
    "        \n",
    "        self.policy = ActorCritic(state_dim, action_dim, action_std_init)\n",
    "        # Adam Optimizer to perform optimization given the loss on NN parameters.\n",
    "        self.optimizer = torch.optim.Adam([\n",
    "            {'params': self.policy.actor.parameters(), 'lr': lr_actor},\n",
    "            {'params': self.policy.critic.parameters(), 'lr': lr_critic}\n",
    "        ])\n",
    "        \n",
    "        # Sampling is always done under old policy(aka old weights) under the evaluation for update step is always done\n",
    "        # under new policy(aka update/new weights). After 800 timestep * Number of Epochs,\n",
    "        # old_policy parameters set same as new_policy parameters. \n",
    "        self.policy_old = ActorCritic(state_dim, action_dim, action_std_init)\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        \n",
    "        # Mean Squared Error \n",
    "        self.MseLoss = nn.MSELoss()\n",
    "\n",
    "    def set_action_std(self, new_action_std):\n",
    "        # Setting new standard deviation.\n",
    "        self.action_std = new_action_std\n",
    "        self.policy.set_action_std(new_action_std)\n",
    "        self.policy_old.set_action_std(new_action_std)\n",
    "\n",
    "    def decay_action_std(self, action_std_decay_rate, min_action_std):\n",
    "        print(\"--------------------------------------------------------------------------------------------\")\n",
    "        # Calculate the new standard deviation corresponding to decay rate.\n",
    "        self.action_std = self.action_std - action_std_decay_rate\n",
    "        self.action_std = round(self.action_std, 2)\n",
    "        # In case, standard deviation decays below threshold, set it to minimum.\n",
    "        if self.action_std <= min_action_std:\n",
    "            self.action_std = min_action_std\n",
    "            print(\"setting actor output action_std to min_action_std : \", self.action_std)\n",
    "        else:\n",
    "            print(\"setting actor output action_std to : \", self.action_std)\n",
    "        self.set_action_std(self.action_std)\n",
    "\n",
    "    def select_action(self, state):\n",
    "        \"\"\"\n",
    "        Selecting action under old policy during sampling phase. At same time, save the state and reward into \n",
    "        rollout buffer to be used during evaluation.\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state)\n",
    "            action, action_logprob = self.policy_old.get_action(state)\n",
    "\n",
    "        self.buffer.states.append(state)\n",
    "        self.buffer.actions.append(action)\n",
    "        self.buffer.logprobs.append(action_logprob)\n",
    "\n",
    "        return action.detach().numpy().flatten()\n",
    "\n",
    "    def update(self):\n",
    "        \"\"\"\n",
    "        Note: Adam is Gradient descent algorithm. Therefore, it tries to find global minimum. For surrogate loss, \n",
    "        gradient ascent is requirred and therefore a negative surrogate loss is used during optimization using\n",
    "        Adam. Maximization of loss function is equal to minimizing a negative loss function.\n",
    "        \"\"\"\n",
    "        # Using Monte Carlo estimates of return we can calcuate the advantage function.\n",
    "        rewards = []\n",
    "        discounted_reward = 0\n",
    "        \n",
    "        # reward for terminal state is zero. Starting from terminal state, add the discounted reward collected\n",
    "        # till the initial state and store them into rewards list. So, no bootstrapping.\n",
    "        \n",
    "        for reward, done in zip(reversed(self.buffer.rewards), reversed(self.buffer.dones)):\n",
    "            if done:\n",
    "                discounted_reward = 0\n",
    "            discounted_reward = reward + (self.gamma * discounted_reward)\n",
    "            rewards.insert(0, discounted_reward)\n",
    "\n",
    "        # By normalizing the rewards, variance in advantage is reduced further.\n",
    "        rewards = torch.tensor(rewards, dtype=torch.float32)\n",
    "        rewards = (rewards - rewards.mean()) / (rewards.std() + 1e-7) # to avoid division by zero.\n",
    "\n",
    "        # The buffer has multiple list. The list as it is cannot be passed to the NN made using pytorch.\n",
    "        # Pytorch works with tensors and therefore, a conversion is done.\n",
    "        \n",
    "        old_states = torch.squeeze(torch.stack(self.buffer.states, dim=0)).detach()\n",
    "        old_actions = torch.squeeze(torch.stack(self.buffer.actions, dim=0)).detach()\n",
    "        old_logprobs = torch.squeeze(torch.stack(self.buffer.logprobs, dim=0)).detach()\n",
    "        \n",
    "        # Using the sampled date from old policy, calculate loss over multiple trajectories and perform optimization.\n",
    "        # Data/ state tuple corresponding to 800 time steps is processed 'K_epochs' times before updating old_policy\n",
    "        # parameters to same as new updated policy parameters.\n",
    "        \n",
    "        for _ in range(self.K_epochs):\n",
    "            logprobs, state_values, policy_entropy = self.policy.evaluate(old_states, old_actions)\n",
    "            state_values = torch.squeeze(state_values)\n",
    "\n",
    "            # Finding the ratio (pi_theta / pi_theta__old)\n",
    "            ratios = torch.exp(logprobs - old_logprobs.detach())\n",
    "\n",
    "            # Finding surrogate loss and then maximizing it. Since, maximization of surrogate loss corresponds to \n",
    "            # increase in prob. of action which gives higher reward given the state. \n",
    "            advantages = rewards - state_values.detach()\n",
    "            surr1 = ratios * advantages\n",
    "            surr2 = torch.clamp(ratios, 1 - self.eps_clip, 1 + self.eps_clip) * advantages\n",
    "            policy_loss = torch.min(surr1, surr2)\n",
    "\n",
    "            # Combined loss of PPO\n",
    "            value_loss = self.MseLoss(state_values, rewards)\n",
    "            \n",
    "            loss = -policy_loss + 0.5 * value_loss - 0.01 * policy_entropy\n",
    "\n",
    "            # Performing Optimization step.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.mean().backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        # Finally, old_policy is same as new _policy until next update phase.\n",
    "        self.policy_old.load_state_dict(self.policy.state_dict())\n",
    "        # Prepare buffer for next round of sampling by clearing all previous entries.\n",
    "        self.buffer.clear()\n",
    "\n",
    "    def save(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Save the weights and biases of old policy to be used later for evaluating PPO performance via rendering the \n",
    "        Environment.\n",
    "        \"\"\"\n",
    "        torch.save(self.policy_old.state_dict(), checkpoint_path)\n",
    "\n",
    "    def load(self, checkpoint_path):\n",
    "        \"\"\"\n",
    "        Load previously trained model parameters for testing.\n",
    "        \"\"\"\n",
    "        self.policy_old.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))\n",
    "        self.policy.load_state_dict(torch.load(checkpoint_path, map_location=lambda storage, loc: storage))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
