{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ddd44a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('C:\\\\Users\\\\Максимилиан\\\\Desktop\\\\Skoltech\\\\Reinforcement learning\\\\Final project')\n",
    "from Dynamics import get_next_state, state_to_coords, get_energy,normalize_angle\n",
    "from Environment import DoublePendulumEnv, ObservationSpaceCartPole\n",
    "from PPO.Proximal_Policy_Optimization import PPO, unscaled_action\n",
    "from PPO.train import train\n",
    "from PPO.config_PPO import config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2ad651c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import gym\n",
    "from torch.distributions import Normal\n",
    "from gym.spaces import Box\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from torch.distributions import MultivariateNormal\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d37f5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e98b69ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "state0 = np.array([0,np.pi/2,np.pi/2,\n",
    "                0,0,0])\n",
    "state = state0\n",
    "state\n",
    "#max_initial_angle = 3 * 2 * np.pi / 360\n",
    "max_initial_angle = 0\n",
    "dt = 0.02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "069330a2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oords = np.array(state_to_coords(state)[:, 0])\n",
    "oords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "638913a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\scipy\\integrate\\odepack.py:247: ODEintWarning: Excess work done on this call (perhaps wrong Dfun type). Run with full_output = 1 to get quantitative information.\n",
      "  warnings.warn(warning_msg, ODEintWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 57.09394275,  -2.53782625,  -2.52245306,  68.56067285,\n",
       "         3.83151235, -23.02406187])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_next_state(state,100,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d455d941",
   "metadata": {},
   "source": [
    "# Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c37e8590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment initialized\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Максимилиан\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "env = DoublePendulumEnv(init_state=state, dt=config['dt'], max_initial_angle = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cfcfaff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe53bb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train():\n",
    "    print(\"============================================================================================\")\n",
    "    # max. timestep per episode. For DoubleCartPoleEnv, time constraint is 200 timesteps. After that environment is\n",
    "    # reset.\n",
    "    max_ep_len = config['max_episode_length']\n",
    "    # The training phase will sample and update for 1 million timestep.\n",
    "    max_training_steps = int(config['max_training_steps'])\n",
    "\n",
    "    # In order, to check ongoing progress, average reward is printed at every 10_000 timesteps.\n",
    "    print_freq = 10_000\n",
    "\n",
    "    # Saving model parameters at every 1_00_000 timesteps.\n",
    "    save_model_freq = int(config['save_model_freq'])\n",
    "\n",
    "    action_std = config['action_std']  # Initial standard deviation.\n",
    "    action_std_decay_rate = config['action_std_decay_rate']  # Decay rate of standard deviation.\n",
    "    min_action_std = config['min_action_std']  # Threshold standard deviation.\n",
    "    action_std_decay_freq = int(2e5)  # Decay the standard deviation every 200000 timesteps\n",
    "\n",
    "    update_timestep = config['update_timestep']  # set old_policy parameters to new_policy parameters.\n",
    "    K_epochs = config['max_number_of_epoch']  # Number of epochs before updating old policy parameters.\n",
    "    eps_clip = config['eps_clip']  # clip range for surrogate loss function.\n",
    "    gamma = config['gamma']  # Discount factor.\n",
    "\n",
    "    lr_actor = config['lr_actor']  # Learning rate for optimizer of actor network.\n",
    "    lr_critic = config['lr_critic']  # Learning rate for optimizer of critic network.\n",
    "    env_name = 'DoubleInvPendulum'\n",
    "    print(\"Training Environment:\" + env_name)\n",
    "    \n",
    "    \n",
    "    \n",
    "    env = DoublePendulumEnv(init_state=state, dt=config['dt'], max_initial_angle = 0)\n",
    "\n",
    "    observation_shape = env.observation_space.shape[0]  # Observation shape\n",
    "    action_shape = env.action_space.shape[0]  # Action shape\n",
    "\n",
    "    directory = \"PPO3_Trained\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "    directory = directory + '/'\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "\n",
    "    checkpoint_path = directory + \"PPO3_{}.pth\".format(env_name)\n",
    "\n",
    "\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"PPO update frequency : \" + str(update_timestep) + \" timesteps\")\n",
    "    print(\"PPO K epochs : \", K_epochs)\n",
    "    print(\"PPO epsilon clip : \", eps_clip)\n",
    "    print(\"discount factor (gamma) : \", gamma)\n",
    "    print(\"--------------------------------------------------------------------------------------------\")\n",
    "    print(\"optimizer learning rate actor : \", lr_actor)\n",
    "    print(\"optimizer learning rate critic : \", lr_critic)\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    agent = PPO(observation_shape,\n",
    "                action_shape,\n",
    "                lr_actor,\n",
    "                lr_critic,\n",
    "                gamma,\n",
    "                K_epochs,\n",
    "                eps_clip,\n",
    "                action_std)\n",
    "\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    # To keep track of the progress\n",
    "    print_running_reward = 0\n",
    "    print_running_episodes = 0\n",
    "\n",
    "    time_step = 0\n",
    "    i_episode = 0\n",
    "    counter = 0\n",
    "\n",
    "    plot_episode = []\n",
    "    plot_reward = []\n",
    "\n",
    "    while time_step <= max_training_steps:\n",
    "        obs, done = env.reset()\n",
    "        current_ep_reward = 0\n",
    "        for t in range(1, max_ep_len + 1):\n",
    "            \n",
    "            u = agent.select_action(obs)  # Get action under old_policy given state.\n",
    "            if u > 1 or u < -1:\n",
    "                print(u)\n",
    "            action = unscaled_action(u)  # Unscale the action.\n",
    "           \n",
    "             \n",
    "            obs, reward, done, _ = env.step(action)  # Apply the action to environment.\n",
    "    \n",
    "\n",
    "            # Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\n",
    "            agent.buffer.rewards.append(reward)\n",
    "            agent.buffer.dones.append(done)\n",
    "            \n",
    "            time_step += 1\n",
    "            current_ep_reward += reward\n",
    "\n",
    "            if time_step % update_timestep == 0:\n",
    "                agent.update()\n",
    "\n",
    "            if time_step % action_std_decay_freq == 0:\n",
    "                # Decay standard deviation by 0.1.\n",
    "                agent.decay_action_std(action_std_decay_rate, min_action_std)\n",
    "\n",
    "            if time_step % print_freq == 0:\n",
    "                # print average reward during 10_000 timesteps\n",
    "                print_avg_reward = print_running_reward / print_running_episodes\n",
    "                print_avg_reward = round(print_avg_reward, 2)\n",
    "\n",
    "                print(\"Episode : {} \\t\\t Timestep : {} \\t\\t Average Reward : {}\".format(i_episode, time_step,\n",
    "                                                                                        print_avg_reward))\n",
    "\n",
    "                print_running_reward = 0\n",
    "                print_running_episodes = 0\n",
    "\n",
    "            if time_step % save_model_freq == 0:\n",
    "\n",
    "                # Save the model parameters for test phase and tracking performance.\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Saving model at : \" + checkpoint_path)\n",
    "                agent.save(checkpoint_path)\n",
    "                print(\"Model saved\")\n",
    "                counter += 1\n",
    "                inter_checkpoint = directory + \"PPO_{}_{}00K.pth\".format(env_name, counter)\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "                print(\"Model parameters to check for intermediate performance saving:.\")\n",
    "                print(\"saving model at : \" + inter_checkpoint)\n",
    "                agent.save(inter_checkpoint)\n",
    "                if counter == 10:\n",
    "                    print(f\"Intermediate model saved for {counter}M\")\n",
    "                else:\n",
    "                    print(f\"Intermediate model saved for {counter}00K\")\n",
    "                print(\"--------------------------------------------------------------------------------------------\")\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        print_running_reward += current_ep_reward\n",
    "\n",
    "        # plot reward per 10 episode.\n",
    "        if i_episode % 10 == 0:\n",
    "            plot_episode.append(i_episode)\n",
    "            plot_reward.append(current_ep_reward)\n",
    "\n",
    "        print_running_episodes += 1\n",
    "\n",
    "        i_episode += 1\n",
    "\n",
    "    env.close()\n",
    "\n",
    "    print(\"============================================================================================\")\n",
    "\n",
    "    return plot_episode, plot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8beec190",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================================================\n",
      "Training Environment:DoubleInvPendulum\n",
      "Environment initialized\n",
      "--------------------------------------------------------------------------------------------\n",
      "PPO update frequency : 2000 timesteps\n",
      "PPO K epochs :  100\n",
      "PPO epsilon clip :  0.2\n",
      "discount factor (gamma) :  0.99\n",
      "--------------------------------------------------------------------------------------------\n",
      "optimizer learning rate actor :  0.0003\n",
      "optimizer learning rate critic :  0.001\n",
      "============================================================================================\n",
      "============================================================================================\n",
      "[1.3025029]\n",
      "[-1.0793355]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Максимилиан\\AppData\\Roaming\\Python\\Python38\\site-packages\\gym\\spaces\\box.py:73: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0221399]\n",
      "[1.7275804]\n",
      "[-1.2659261]\n",
      "[1.0589556]\n",
      "[1.5166891]\n",
      "[1.049843]\n",
      "[1.3105762]\n",
      "[-1.3961219]\n",
      "[-1.4365257]\n",
      "[-1.1729294]\n",
      "[-1.0304153]\n",
      "[1.5369287]\n",
      "[-1.1822482]\n",
      "[1.2807575]\n",
      "[1.0735373]\n",
      "[-1.195234]\n",
      "[1.778472]\n",
      "[1.2622848]\n",
      "[-1.632945]\n",
      "[1.0145168]\n",
      "[1.4179134]\n",
      "[-1.3264284]\n",
      "[-1.3418809]\n",
      "[-1.0625126]\n",
      "[-1.2078282]\n",
      "[1.3500963]\n",
      "[-1.0036799]\n",
      "[-1.1666349]\n",
      "[1.0019734]\n",
      "[-1.1961957]\n",
      "[1.3622625]\n",
      "[-1.512189]\n",
      "[-1.2303703]\n",
      "[-1.6332834]\n",
      "[-1.7220224]\n",
      "[-1.3772359]\n",
      "[1.0429773]\n",
      "[1.3201408]\n",
      "[1.5751592]\n",
      "[1.8113781]\n",
      "[-1.5559007]\n",
      "[1.2665517]\n",
      "[-1.1550573]\n",
      "[1.5191438]\n",
      "[-2.134606]\n",
      "[1.7971187]\n",
      "[-1.0552176]\n",
      "[-1.5633999]\n",
      "[-1.7455708]\n",
      "[1.0650923]\n",
      "[-1.0013759]\n",
      "[-1.5362703]\n",
      "[1.1472696]\n",
      "[1.0376921]\n",
      "[-1.0131884]\n",
      "[-1.149447]\n",
      "[1.2576213]\n",
      "[1.116726]\n",
      "[1.9742234]\n",
      "[1.2842555]\n",
      "[1.1051769]\n",
      "[-1.1618147]\n",
      "[-1.4329447]\n",
      "[1.4035187]\n",
      "[-1.1254133]\n",
      "[1.0038574]\n",
      "[1.4956261]\n",
      "[1.4167312]\n",
      "[-1.1227685]\n",
      "[1.6049358]\n",
      "[-1.6219281]\n",
      "[-1.04529]\n",
      "[1.2014908]\n",
      "[-1.196336]\n",
      "[-1.027134]\n",
      "[-1.198491]\n",
      "[1.138653]\n",
      "[1.2013947]\n",
      "[-1.5009279]\n",
      "[-1.4310951]\n",
      "[1.1242049]\n",
      "[-1.2329693]\n",
      "[1.0378385]\n",
      "[1.059662]\n",
      "[1.1183302]\n",
      "[1.1697311]\n",
      "[1.4277667]\n",
      "[1.1288322]\n",
      "[1.1373498]\n",
      "[-1.6283013]\n",
      "[-1.1802465]\n",
      "[-1.2326881]\n",
      "[1.2924985]\n",
      "[-1.0439909]\n",
      "[1.1798894]\n",
      "[2.026811]\n",
      "[1.5663527]\n",
      "[-1.5130905]\n",
      "[1.0917736]\n",
      "[1.1153361]\n",
      "[1.0796762]\n",
      "[-1.1733952]\n",
      "[-1.0883727]\n",
      "[1.6948706]\n",
      "[-1.2075549]\n",
      "[1.1424131]\n",
      "[1.1540385]\n",
      "[-1.1499298]\n",
      "[-1.1344783]\n",
      "[1.5793927]\n",
      "[-1.0023555]\n",
      "[-1.0995277]\n",
      "[1.1613119]\n",
      "[1.2829437]\n",
      "[1.0982687]\n",
      "[-1.6877435]\n",
      "[1.2584301]\n",
      "[1.3976766]\n",
      "[-1.3643762]\n",
      "[1.1168]\n",
      "[-1.1494491]\n",
      "[1.4479862]\n",
      "[1.6220229]\n",
      "[-1.1470642]\n",
      "[-1.0526478]\n",
      "[1.6700432]\n",
      "[-1.1644461]\n",
      "[1.258505]\n",
      "[1.0762054]\n",
      "[-1.2955768]\n",
      "[-1.0421426]\n",
      "[-1.73727]\n",
      "[1.0151991]\n",
      "[-1.2676157]\n",
      "[-1.1331686]\n",
      "[1.0064051]\n",
      "[1.0420878]\n",
      "[1.0310348]\n",
      "[-1.0357573]\n",
      "[-1.4170834]\n",
      "[-1.2972802]\n",
      "[1.3528966]\n",
      "[1.3674109]\n",
      "[-1.1266465]\n",
      "[1.1056337]\n",
      "[1.4786719]\n",
      "[-1.2367597]\n",
      "[-1.1769713]\n",
      "[1.8640746]\n",
      "[-1.3778512]\n",
      "[-1.1707114]\n",
      "[1.0688746]\n",
      "[-1.6803646]\n",
      "[1.0578033]\n",
      "[1.6713169]\n",
      "[-1.5017103]\n",
      "[1.3283978]\n",
      "[-1.0913445]\n",
      "[1.3724777]\n",
      "[1.2467215]\n",
      "[1.2055799]\n",
      "[-1.2045745]\n",
      "[1.2864366]\n",
      "[1.103792]\n",
      "[1.4672852]\n",
      "[-1.2427754]\n",
      "[-1.335762]\n",
      "[1.0198855]\n",
      "[1.2391388]\n",
      "[1.1876135]\n",
      "[1.312591]\n",
      "[1.0054286]\n",
      "[1.3207382]\n",
      "[1.3029203]\n",
      "[1.0290451]\n",
      "[-1.4403881]\n",
      "[1.071347]\n",
      "[-1.326008]\n",
      "[1.5812286]\n",
      "[-1.4268712]\n",
      "[-1.3552271]\n",
      "[1.0629166]\n",
      "[-1.0286648]\n",
      "[1.2976328]\n",
      "[-1.5464419]\n",
      "[-1.1729046]\n",
      "[1.0251958]\n",
      "[1.0218993]\n",
      "[1.7215513]\n",
      "[1.0640264]\n",
      "[-1.5531693]\n",
      "[-1.2117648]\n",
      "[1.3740464]\n",
      "[-1.3961864]\n",
      "[-1.1965162]\n",
      "[-1.4776939]\n",
      "[1.4548569]\n",
      "[-1.5741835]\n",
      "[-1.1956861]\n",
      "[-1.381192]\n",
      "[-1.0588534]\n",
      "[-1.7577382]\n",
      "[-1.0855645]\n",
      "[-1.393993]\n",
      "[2.1790676]\n",
      "[1.0848728]\n",
      "[1.2443744]\n",
      "[1.0038459]\n",
      "[-1.3354201]\n",
      "[1.7055429]\n",
      "[1.2135208]\n",
      "[1.4149758]\n",
      "[1.0862589]\n",
      "[-1.0882]\n",
      "[1.0198097]\n",
      "[1.2194241]\n",
      "[1.1693251]\n",
      "[1.3779712]\n",
      "[1.1031874]\n",
      "[-1.2134267]\n",
      "[1.0008979]\n",
      "[1.1201782]\n",
      "[1.1689491]\n",
      "[1.2275509]\n",
      "[1.1030463]\n",
      "[1.0300366]\n",
      "[1.3284345]\n",
      "[1.4932104]\n",
      "[1.4138348]\n",
      "[1.6839386]\n",
      "[1.8052392]\n",
      "[-1.0711782]\n",
      "[1.2875472]\n",
      "[1.116515]\n",
      "[1.2749479]\n",
      "[1.2073282]\n",
      "[1.0945971]\n",
      "[-1.4741985]\n",
      "[1.5006895]\n",
      "[-1.257004]\n",
      "[1.2726752]\n",
      "[1.3757663]\n",
      "[-1.2392833]\n",
      "[-1.0418428]\n",
      "[-1.0611134]\n",
      "[1.0898268]\n",
      "[-1.0083965]\n",
      "[-1.8116609]\n",
      "[-1.4254924]\n",
      "[-1.1410358]\n",
      "[1.3105125]\n",
      "[1.0899746]\n",
      "[-1.398565]\n",
      "[-1.038395]\n",
      "[-1.1097608]\n",
      "[-1.489358]\n",
      "[1.0497942]\n",
      "[1.2897606]\n",
      "[-1.0571991]\n",
      "[-1.3649572]\n",
      "[1.4629569]\n",
      "[-1.2891331]\n",
      "[1.0371113]\n",
      "[-1.1153908]\n",
      "[1.1492585]\n",
      "[-1.0711533]\n",
      "[1.1269625]\n",
      "[-1.2008158]\n",
      "[-1.0826759]\n",
      "[1.2334424]\n",
      "[1.1308013]\n",
      "[-1.1723129]\n",
      "[-1.2561356]\n",
      "[1.3541839]\n",
      "[1.2391361]\n",
      "[-1.0231562]\n",
      "[1.1853201]\n",
      "[1.1020368]\n",
      "[1.4447912]\n",
      "[-1.0221777]\n",
      "[-1.2561965]\n",
      "[1.8499285]\n",
      "[1.2616113]\n",
      "[1.4190878]\n",
      "[-1.5466155]\n",
      "[-1.5008801]\n",
      "[-1.0625968]\n",
      "[1.0123894]\n",
      "[1.0562938]\n",
      "[-1.3380305]\n",
      "[-1.0205568]\n",
      "[-1.3675501]\n",
      "[1.3510225]\n",
      "[-1.1762046]\n",
      "[1.0470504]\n",
      "[1.0381719]\n",
      "[1.1640239]\n",
      "[-1.2861868]\n",
      "[1.0004427]\n",
      "[1.1887064]\n",
      "[1.0183867]\n",
      "[1.0078284]\n",
      "[1.1181153]\n",
      "[1.0397031]\n",
      "[-1.2068707]\n",
      "[-1.4802421]\n",
      "[-1.0422692]\n",
      "[-1.7357699]\n",
      "[1.1401359]\n",
      "[1.2031467]\n",
      "[1.1906033]\n",
      "[1.151906]\n",
      "[-1.2413359]\n",
      "[-1.0510713]\n",
      "[-1.2577838]\n",
      "[-2.4965854]\n",
      "[-1.002066]\n",
      "[-1.535039]\n",
      "[1.1343212]\n",
      "[1.0953246]\n",
      "[1.5091588]\n",
      "[1.5247097]\n",
      "[1.1453893]\n",
      "[1.6398666]\n",
      "[-1.0426257]\n",
      "[1.2288285]\n",
      "[-1.0113398]\n",
      "[1.1277862]\n",
      "[-1.3213452]\n",
      "[-1.0068581]\n",
      "[-1.1375142]\n",
      "[1.1107169]\n",
      "[-1.2904748]\n",
      "[-1.4205463]\n",
      "[-1.0771173]\n",
      "[-1.0057414]\n",
      "[1.1408236]\n",
      "[-1.2980919]\n",
      "[1.1113288]\n",
      "[1.5942011]\n",
      "[-1.007411]\n",
      "[1.1946106]\n",
      "[-1.3078471]\n",
      "[-1.0930861]\n",
      "[1.0651145]\n",
      "[-1.6551325]\n",
      "[1.2419349]\n",
      "[-1.167044]\n",
      "[-1.2418098]\n",
      "[1.0484972]\n",
      "[-1.0410087]\n",
      "[-1.0165241]\n",
      "[-1.3115617]\n",
      "[-1.1497933]\n",
      "[-1.1032342]\n",
      "[-1.1156615]\n",
      "[-1.100416]\n",
      "[-1.3477398]\n",
      "[1.0102383]\n",
      "[-1.0310704]\n",
      "[-1.0225296]\n",
      "[-1.0201921]\n",
      "[1.0496998]\n",
      "[1.3989294]\n",
      "[-1.114372]\n",
      "[1.2259799]\n",
      "[1.784272]\n",
      "[-1.1660525]\n",
      "[1.304071]\n",
      "[1.0099254]\n",
      "[1.447919]\n",
      "[1.0485882]\n",
      "[1.0660468]\n",
      "[1.5407381]\n",
      "[-1.0070537]\n",
      "[1.5674058]\n",
      "[1.2819788]\n",
      "[1.0408058]\n",
      "[1.0839273]\n",
      "[1.5988964]\n",
      "[1.0424652]\n",
      "[-1.0676986]\n",
      "[-1.9939355]\n",
      "[-1.5950875]\n",
      "[-1.2979895]\n",
      "[-1.1222272]\n",
      "[1.1499441]\n",
      "[-1.153704]\n",
      "[-1.5904613]\n",
      "[-1.1669905]\n",
      "[1.2143528]\n",
      "[-1.6178675]\n",
      "[-1.2368512]\n",
      "[-1.014971]\n",
      "[1.0187037]\n",
      "[-1.1502832]\n",
      "[-1.1842332]\n",
      "[-1.1718469]\n",
      "[-1.6893884]\n",
      "[1.579456]\n",
      "[1.5539894]\n",
      "[-1.2173451]\n",
      "[-1.0678622]\n",
      "[-1.218161]\n",
      "[1.1808892]\n",
      "[1.541012]\n",
      "[1.2541422]\n",
      "[-1.1764545]\n",
      "[1.1495746]\n",
      "[1.1910423]\n",
      "[1.2075585]\n",
      "[-1.4930947]\n",
      "[-1.0803938]\n",
      "[1.2060008]\n",
      "[-1.1704258]\n",
      "[1.5929117]\n",
      "[1.2009764]\n",
      "[1.0199033]\n",
      "[1.2330451]\n",
      "[-1.1055723]\n",
      "[1.6631261]\n",
      "[1.1009784]\n",
      "[1.1196272]\n",
      "[-1.1207356]\n",
      "[-1.170046]\n",
      "[-1.1145005]\n",
      "[-1.1280465]\n",
      "[-1.4266189]\n",
      "[1.0854896]\n",
      "[1.524638]\n",
      "[1.257994]\n",
      "[-1.2374792]\n",
      "[-1.090244]\n",
      "[-1.0654589]\n",
      "[-1.0582166]\n",
      "[-1.1781908]\n",
      "[1.4959426]\n",
      "[-1.0667229]\n",
      "[-1.3529133]\n",
      "[1.0726506]\n",
      "[1.8709348]\n",
      "[1.0902829]\n",
      "[1.4429245]\n",
      "[-1.4783523]\n",
      "[1.2254837]\n",
      "[-1.3198804]\n",
      "[-1.0104864]\n",
      "[-1.0367011]\n",
      "[-1.0933375]\n",
      "[-1.0512096]\n",
      "[1.2226347]\n",
      "[1.1641589]\n",
      "[1.1118243]\n",
      "[1.0694757]\n",
      "[-1.1560669]\n",
      "[-1.077724]\n",
      "[1.0527056]\n",
      "[1.9647982]\n",
      "[1.0322536]\n",
      "[1.0415119]\n",
      "[-1.0474132]\n",
      "[1.6810551]\n",
      "[1.0470178]\n",
      "[1.7041473]\n",
      "[1.7097067]\n",
      "[1.2360637]\n",
      "[1.1505377]\n",
      "[1.0804102]\n",
      "[-1.493844]\n",
      "[1.0239587]\n",
      "[1.4199003]\n",
      "[1.0262975]\n",
      "[-1.1178668]\n",
      "[1.3140444]\n",
      "[1.159822]\n",
      "[-1.0092474]\n",
      "[1.1443788]\n",
      "[1.2729726]\n",
      "[1.0098658]\n",
      "[1.3069272]\n",
      "[1.0691227]\n",
      "[1.1154234]\n",
      "[1.0725247]\n",
      "[1.232193]\n",
      "[-1.111016]\n",
      "[-1.2093666]\n",
      "[-1.0579615]\n",
      "[-1.1076839]\n",
      "[-1.3013153]\n",
      "[1.1273682]\n",
      "[1.075973]\n",
      "[1.0812603]\n",
      "[-1.0302216]\n",
      "[1.3564303]\n",
      "[1.2100401]\n",
      "[-1.3812225]\n",
      "[1.3142174]\n",
      "[1.5933502]\n",
      "[-1.6650023]\n",
      "[-1.2226149]\n",
      "[-1.4701495]\n",
      "[1.4530666]\n",
      "[1.9755808]\n",
      "[1.0104923]\n",
      "[-1.2647327]\n",
      "[-1.0528507]\n",
      "[-1.220421]\n",
      "[1.0586115]\n",
      "[-1.2215635]\n",
      "[-1.5755891]\n",
      "[1.0423282]\n",
      "[-1.3608862]\n",
      "[-1.547563]\n",
      "[-1.3996992]\n",
      "[-1.3522582]\n",
      "[1.015875]\n",
      "[-1.4012353]\n",
      "[-1.1703259]\n",
      "[-1.7436172]\n",
      "[-1.0475241]\n",
      "[1.0070066]\n",
      "[1.0346034]\n",
      "[1.1436081]\n",
      "[-1.1145841]\n",
      "[1.3471633]\n",
      "[1.2045468]\n",
      "[1.1549445]\n",
      "[-1.2122515]\n",
      "[1.4363731]\n",
      "[-1.2961296]\n",
      "[1.0665418]\n",
      "[-1.0079467]\n",
      "[1.3223771]\n",
      "[1.0393685]\n",
      "[1.0734901]\n",
      "[1.2985475]\n",
      "[1.0174118]\n",
      "[-1.10019]\n",
      "[1.001148]\n",
      "[1.0235969]\n",
      "[1.7789011]\n",
      "[1.1757169]\n",
      "[1.191322]\n",
      "[1.0928804]\n",
      "[1.0509658]\n",
      "[1.6188934]\n",
      "[1.1237442]\n",
      "[1.30446]\n",
      "[-1.090689]\n",
      "[-1.1799126]\n",
      "[-1.396491]\n",
      "[-1.047434]\n",
      "[-1.2326665]\n",
      "[1.150181]\n",
      "[1.5215143]\n",
      "[1.9135513]\n",
      "[-1.0139986]\n",
      "[-1.2777575]\n",
      "[-1.0534538]\n",
      "[1.0466948]\n",
      "[1.0676862]\n",
      "[1.0495328]\n",
      "[-1.1408515]\n",
      "[1.2223375]\n",
      "[1.3983146]\n",
      "[-1.0613782]\n",
      "[-1.2608705]\n",
      "[-1.4420977]\n",
      "[1.1211417]\n",
      "[1.1638569]\n",
      "[-1.5151957]\n",
      "[-1.1130804]\n",
      "[-2.175667]\n",
      "[1.3617277]\n",
      "[-1.0502504]\n",
      "[1.1634885]\n",
      "[-1.0324123]\n",
      "[-1.0819287]\n",
      "[2.1400478]\n",
      "[-1.1340793]\n",
      "[1.5245228]\n",
      "[1.2237576]\n",
      "[-1.3694521]\n",
      "[1.0425649]\n",
      "[-1.0211096]\n",
      "[1.5073025]\n",
      "[-1.3499941]\n",
      "[-1.1133211]\n",
      "[1.0079854]\n",
      "[1.1785314]\n",
      "[-1.0664871]\n",
      "[1.078567]\n",
      "[-1.0417153]\n",
      "[-1.3306792]\n",
      "[1.186708]\n",
      "[-1.0121589]\n",
      "[1.123805]\n",
      "[-1.2531803]\n",
      "[1.2851366]\n",
      "[-1.2494385]\n",
      "[1.6525012]\n",
      "[1.5381323]\n",
      "[1.6622488]\n",
      "[-1.5447032]\n",
      "[-1.0841296]\n",
      "[-1.0132614]\n",
      "[1.1963985]\n",
      "[-1.0362587]\n",
      "[1.0641905]\n",
      "[1.1604322]\n",
      "[1.2275953]\n",
      "[1.0024971]\n",
      "[-1.0647738]\n",
      "[-1.3428879]\n",
      "[-1.0877161]\n",
      "[1.2702625]\n",
      "[-1.4034929]\n",
      "[-1.2900308]\n",
      "[1.0294379]\n",
      "[-1.1248258]\n",
      "[1.1120709]\n",
      "[1.2942904]\n",
      "[1.1088398]\n",
      "[1.3298703]\n",
      "[1.595583]\n",
      "[-1.1188267]\n",
      "[1.3991302]\n",
      "[1.3958201]\n",
      "[-1.3286159]\n",
      "[-1.0703361]\n",
      "[-1.2483162]\n",
      "[1.131669]\n",
      "[-1.1071874]\n",
      "[-1.0223298]\n",
      "[-1.3992887]\n",
      "[1.2465805]\n",
      "[1.0192962]\n",
      "[-1.7489312]\n",
      "[-1.9624051]\n",
      "[1.4521858]\n",
      "[1.9639598]\n",
      "[1.0266045]\n",
      "[1.1620331]\n",
      "[-1.2535781]\n",
      "[1.1052547]\n",
      "[-1.0009528]\n",
      "[1.6253543]\n",
      "[1.2429568]\n",
      "[1.5212361]\n",
      "[-1.1345403]\n",
      "[1.0513225]\n",
      "[-1.168651]\n",
      "[-1.2179649]\n",
      "[-1.1119542]\n",
      "[1.0375811]\n",
      "[-1.4585586]\n",
      "[1.0159215]\n",
      "[1.3212867]\n",
      "[1.0157498]\n",
      "[1.1658642]\n",
      "[1.0060942]\n",
      "[-1.1416724]\n",
      "[1.1664643]\n",
      "[1.094126]\n",
      "[-1.3399026]\n",
      "[1.1222789]\n",
      "[-1.0649179]\n",
      "[-1.0560441]\n",
      "[1.0523405]\n",
      "[-1.049748]\n",
      "[1.0801787]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.095363]\n",
      "[1.8948252]\n",
      "[1.403856]\n",
      "[-1.1551745]\n",
      "[-1.213865]\n",
      "[-1.0429785]\n",
      "[1.1066171]\n",
      "[1.1428282]\n",
      "[1.6874892]\n",
      "[-1.100004]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [12]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m n_episode, reward_episode \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m     90\u001b[0m     \u001b[38;5;28mprint\u001b[39m(u)\n\u001b[0;32m     91\u001b[0m action \u001b[38;5;241m=\u001b[39m unscaled_action(u)  \u001b[38;5;66;03m# Unscale the action.\u001b[39;00m\n\u001b[1;32m---> 94\u001b[0m obs, reward, done, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply the action to environment.\u001b[39;00m\n\u001b[0;32m     97\u001b[0m \u001b[38;5;66;03m# Append the reward and done flag to buffer for calculating Monte Carlo returns during updating phase.\u001b[39;00m\n\u001b[0;32m     98\u001b[0m agent\u001b[38;5;241m.\u001b[39mbuffer\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(reward)\n",
      "File \u001b[1;32m~\\Desktop\\Skoltech\\Reinforcement learning\\Final project\\Environment.py:162\u001b[0m, in \u001b[0;36mDoublePendulumEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_history\u001b[38;5;241m.\u001b[39mappend(action)\n\u001b[0;32m    161\u001b[0m reward, done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reward_function()\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, reward, done, info\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "n_episode, reward_episode = train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cac7b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir = 'PPO_plots'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601c7378",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot: Reward per every 10 episodes.\n",
    "fig, ax = plt.subplots(figsize=(15, 10))\n",
    "ax.plot(n_episode, reward_episode)\n",
    "ax.set_title('Total number of episodes vs rewards per episode', fontsize=20)\n",
    "ax.set_xlabel('Episode', fontsize=20)\n",
    "ax.set_ylabel('Reward', fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bf3499",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import animation\n",
    "%matplotlib widget\n",
    "\n",
    "state = state0\n",
    "\n",
    "dt = 0.1\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal', autoscale_on=False,\n",
    "                     xlim=(-2, 2), ylim=(-2, 2))\n",
    "ax.grid()\n",
    "\n",
    "line, = ax.plot([], [], 'o-', lw=2)\n",
    "energy_text = ax.text(0.02, 0.90, '', transform=ax.transAxes)\n",
    "\n",
    "def init():\n",
    "    \"\"\"initialize animation\"\"\"\n",
    "    line.set_data([], [])\n",
    "    #time_text.set_text('')\n",
    "    energy_text.set_text('')\n",
    "    return line\n",
    "\n",
    "\n",
    "def animate(i):\n",
    "    \"\"\"perform animation step\"\"\"\n",
    "    global state, dt\n",
    "    state_t = torch.FloatTensor(state)\n",
    "    u = agent.select_action(state_t)\n",
    "    action = unscaled_action(u, action_low = -100, action_high = 100)\n",
    "    state = get_next_state(state,u,dt)\n",
    "    XY = state_to_coords(state)\n",
    "    en = get_energy(state)\n",
    "    \n",
    "    line.set_data(XY[0],XY[1])\n",
    "    energy_text.set_text(f'energy = {en}')\n",
    "    return line,\n",
    "\n",
    "ani = animation.FuncAnimation(fig, animate, frames=100,\n",
    "                             interval=10, blit=True, init_func=init)\n",
    "\n",
    "from IPython.display import HTML\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9466af0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
